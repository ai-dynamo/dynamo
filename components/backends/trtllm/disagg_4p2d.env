# export IMAGE="nvcr.io/nvidia/ai-dynamo/tensorrtllm-runtime:0.4.0"
export IMAGE="/lustre/fsw/coreai_tritoninference_triton3/rmccormick/images/dynamo-trtllm-0.4.0-amd64.sqsh"

# Assumes $PWD is dynamo/components/backends/trtllm 
export MOUNTS="${PWD}:/mnt,/lustre:/lustre"

# JET path
export MODEL_PATH="/lustre/share/coreai_dlalgo_ci/artifacts/model/llama-4-maverick_17b_128e_pyt/safetensors_mode-instruct/hf-e91306a-dynamo-fp8/"
export SERVED_MODEL_NAME="nvidia/Llama-4-Maverick-17B-128E-Instruct-FP8"

# DEBUG: Small model for quick sanity checks
# export MODEL_PATH="/lustre/share/coreai_dlalgo_ci/artifacts/model/qwen3_0.6b_pyt/safetensors_mode-instruct/hf-7765feb-nim-fp8/"
# export SERVED_MODEL_NAME="Qwen/Qwen3-0.6B"

# export ENGINE_CONFIG="/mnt/engine_configs/llama4/agg.yaml"
export PREFILL_ENGINE_CONFIG="/mnt/engine_configs/llama4/prefill.yaml"
export DECODE_ENGINE_CONFIG="/mnt/engine_configs/llama4/decode.yaml"

export NUM_GPUS_PER_NODE=8
export NUM_PREFILL_NODES=1
export NUM_DECODE_NODES=1

export NUM_PREFILL_WORKERS=4
export NUM_DECODE_WORKERS=2

export DISAGGREGATION_STRATEGY="prefill_first"
