# SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0
#
# DynamoGraphDeploymentRequest for MoE model profiling
# Converted from profile_sla_moe_job.yaml
apiVersion: nvidia.com/v1alpha1
kind: DynamoGraphDeploymentRequest
metadata:
  name: sla-moe
spec:
  modelName: deepseek-ai/DeepSeek-R1
  backend: sglang

  # ProfilingConfig maps directly to the profile_sla.py config format
  profilingConfig:
    config:
      # Engine configuration
      engine:
        is_moe_model: true  # Enable MoE model support (uses TEP/DEP instead of TP)

      # Sweep/profiling configuration
      sweep:
        # Standard online profiling (not using AI Configurator)
        use_ai_configurator: false

      # SLA targets for profiling
      sla:
        isl: 3000   # Input sequence length
        osl: 150    # Output sequence length
        ttft: 200.0 # Time To First Token target (milliseconds)
        itl: 20.0   # Inter-Token Latency target (milliseconds)

    # Reference to ConfigMap containing the DGD base config
    # For MoE models, this should point to the appropriate disagg config
    # Original path: /sgl-workspace/dynamo/recipes/deepseek-r1/sglang-wideep/tep16p-dep16d-disagg.yaml
    configMapRef:
      name: deepseek-r1-config
      key: tep16p-dep16d-disagg.yaml

  # Automatically create DynamoGraphDeployment after profiling
  autoApply: true

