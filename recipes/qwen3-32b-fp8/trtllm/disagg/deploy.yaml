# SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0
apiVersion: nvidia.com/v1alpha1
kind: DynamoGraphDeployment
metadata:
  name: qwen3-32b-fp8-disagg
spec:
  backendFramework: trtllm
  pvcs:
    - name: model-cache-qwen3-32b
      create: false
  services:
    Frontend:
      componentType: frontend
      dynamoNamespace: qwen3-32b-fp8-disagg
      extraPodSpec:
        affinity:
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                - key: nvidia.com/dynamo-graph-deployment-name
                  operator: In
                  values:
                  - qwen3-32b-fp8-disagg-frontend
              topologyKey: kubernetes.io/hostname
        mainContainer:
          args:
          - python3 -m dynamo.frontend --router-mode round-robin --http-port 8000
          command:
          - /bin/sh
          - -c
          image: my-registry/trtllm-runtime:my-tag
      replicas: 1
    TrtllmPrefillWorker:
      componentType: worker
      subComponentType: prefill
      dynamoNamespace: qwen3-32b-fp8-disagg
      envFromSecret: hf-token-secret
      volumeMounts:
        - name: model-cache-qwen3-32b
          mountPoint: /root/.cache/huggingface
      sharedMemory:
        size: 80Gi
      extraPodSpec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: nvidia.com/gpu.present
                  operator: In
                  values:
                  - "true"
        mainContainer:
          args:
          - |
            python3 -m dynamo.trtllm \
              --model-path "${MODEL_PATH}" \
              --served-model-name "qwen/qwen3-32Bb-fp8" \
              --extra-engine-args "${ENGINE_ARGS}" \
              --tensor-parallel-size 1 \
              --max-batch-size 1 \
              --free-gpu-memory-fraction 0.9 \
              --disaggregation-mode prefill \
              --disaggregation-strategy prefill_first
          command:
          - /bin/sh
          - -c
          image: my-registry/trtllm-runtime:my-tag
          env:
          - name: TRTLLM_ENABLE_PDL
            value: "1"
          - name: TRT_LLM_DISABLE_LOAD_WEIGHTS_IN_PARALLEL
            value: "True"
          - name: SERVED_MODEL_NAME
            value: "qwen/qwen3-32Bb-fp8"
          - name: ENGINE_ARGS
            value: "/opt/dynamo/configs/config-prefill.yaml"
          - name: MODEL_PATH
            value: "/model-store/hub/models--qwen--qwen3-32b-fp8/snapshots/aa55da1ecc13d006e8b8e4f54579b1ea8c3db2df"
          volumeMounts:
          - mountPath: /opt/dynamo/configs
            name: llm-config-prefill
            readOnly: true
          workingDir: /workspace/components/backends/trtllm
        volumes:
        - configMap:
            name: llm-config-prefill
          name: llm-config-prefill
      replicas: 4
      resources:
        limits:
          gpu: "1"
        requests:
          gpu: "1"
    TrtllmDecodeWorker:
      componentType: worker
      subComponentType: decode
      dynamoNamespace: qwen3-32b-fp8-disagg
      envFromSecret: hf-token-secret
      volumeMounts:
        - name: model-cache-qwen3-32b
          mountPoint: /root/.cache/huggingface
      sharedMemory:
        size: 80Gi
      extraPodSpec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: nvidia.com/gpu.present
                  operator: In
                  values:
                  - "true"
        mainContainer:
          args:
          - |
            python3 -m dynamo.trtllm \
              --model-path "${MODEL_PATH}" \
              --served-model-name "qwen/qwen3-32Bb-fp8" \
              --extra-engine-args "${ENGINE_ARGS}" \
              --tensor-parallel-size 2 \
              --max-batch-size 128 \
              --free-gpu-memory-fraction 0.9 \
              --disaggregation-mode decode \
              --disaggregation-strategy prefill_first
          command:
          - /bin/sh
          - -c
          image: my-registry/trtllm-runtime:my-tag
          env:
          - name: TRTLLM_ENABLE_PDL
            value: "1"
          - name: TRT_LLM_DISABLE_LOAD_WEIGHTS_IN_PARALLEL
            value: "True"
          - name: SERVED_MODEL_NAME
            value: "qwen/qwen3-32Bb-fp8"
          - name: ENGINE_ARGS
            value: "/opt/dynamo/configs/config-decode.yaml"
          - name: MODEL_PATH
            value: "/model-store/hub/models--qwen--qwen3-32b-fp8/snapshots/aa55da1ecc13d006e8b8e4f54579b1ea8c3db2df"
          volumeMounts:
          - mountPath: /opt/dynamo/configs
            name: llm-config-decode
            readOnly: true
          workingDir: /workspace/components/backends/trtllm
        volumes:
        - configMap:
            name: llm-config-decode
          name: llm-config-decode
      replicas: 2
      resources:
        limits:
          gpu: "2"
        requests:
          gpu: "2"
