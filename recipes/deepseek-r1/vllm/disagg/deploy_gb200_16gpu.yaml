# SPDX-FileCopyrightText: Copyright (c) 2025-2026 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0

# DeepSeek-R1 (FP4) with vLLM â€” Disaggregated Prefill/Decode on 16x GB200
#
# Architecture: 4 GB200 nodes (4 GPUs each), 16 GPUs total
#   - Prefill: 8 GPUs (2 nodes), DP+EP with AllGather-ReduceScatter (MNNVL-native)
#   - Decode:  8 GPUs (2 nodes), DP+EP with AllGather-ReduceScatter (MNNVL-native)
#
# Uses NVIDIA's FP4-quantized DeepSeek-R1 checkpoint for native Blackwell
# FP4 tensor core support. Achieves ~26K prefill TPGS and ~10K decode TPGS.
#
# Prerequisites:
#   - 4x GB200 nodes with NVLink-C2C connectivity
#   - Model weights downloaded to PVC at /model-cache/deepseek-r1-fp4
#     (use recipes/deepseek-r1/model-cache/ to provision)

# ComputeDomain provides IMEX channel access for MNNVL (multi-node NVLink)
# numNodes = total nodes across all workers (prefill + decode nodeCount)
apiVersion: resource.nvidia.com/v1beta1
kind: ComputeDomain
metadata:
  name: vllm-dsr1-gb200-compute-domain
spec:
  numNodes: 4
  channel:
    resourceClaimTemplate:
      name: vllm-dsr1-gb200-compute-domain-channel
---

apiVersion: nvidia.com/v1alpha1
kind: DynamoGraphDeployment
metadata:
  name: vllm-dsr1-gb200
spec:
  backendFramework: vllm
  pvcs:
    - name: model-cache
      create: false
  services:
    Frontend:
      componentType: frontend
      replicas: 1
      volumeMounts:
        - name: model-cache
          mountPoint: /model-cache
      extraPodSpec:
        containers: []
        imagePullSecrets:
          - name: nvcr-pull-secret
        tolerations:
          - key: "dedicated"
            operator: "Equal"
            value: "user-workload"
            effect: "NoSchedule"
          - key: "dedicated"
            operator: "Equal"
            value: "user-workload"
            effect: "NoExecute"
        mainContainer:
          startupProbe:
            httpGet:
              path: /health
              port: 8000
            periodSeconds: 10
            timeoutSeconds: 1800
            failureThreshold: 60
          image: nvcr.io/nvidian/dynamo-dev/vllm-runtime:ptarasiewicz-dsr1-gb200-arm64
    decode:
      componentType: worker
      subComponentType: decode
      replicas: 1
      multinode:
        nodeCount: 2
      resources:
        requests:
          cpu: "130"
          memory: "850Gi"
        limits:
          cpu: "130"
          memory: "850Gi"
          gpu: "4"
        claims:
          - name: compute-domain-channel
      volumeMounts:
        - name: model-cache
          mountPoint: /model-cache
      sharedMemory:
        size: 800Gi
      extraPodSpec:
        containers: []
        imagePullSecrets:
          - name: nvcr-pull-secret
        resourceClaims:
          - name: compute-domain-channel
            resourceClaimTemplateName: vllm-dsr1-gb200-compute-domain-channel
        tolerations:
          - key: "dedicated"
            operator: "Equal"
            value: "user-workload"
            effect: "NoSchedule"
          - key: "dedicated"
            operator: "Equal"
            value: "user-workload"
            effect: "NoExecute"
        mainContainer:
          startupProbe:
            httpGet:
              path: /health
              port: 9090
            periodSeconds: 10
            timeoutSeconds: 10
            failureThreshold: 600
          image: nvcr.io/nvidian/dynamo-dev/vllm-runtime:ptarasiewicz-dsr1-gb200-arm64
          workingDir: /workspace/dynamo
          env:
            # --- Blackwell FP4 kernel optimizations ---
            - name: VLLM_USE_FLASHINFER_MOE_FP4
              value: "1"
            - name: VLLM_USE_TRTLLM_RAGGED_DEEPSEEK_PREFILL
              value: "1"
            - name: VLLM_USE_NCCL_SYMM_MEM
              value: "1"
            # --- MoE / Expert Parallel ---
            - name: VLLM_SKIP_P2P_CHECK
              value: "1"
            # --- NVLink-C2C / Multi-node NVLink ---
            - name: NCCL_MNNVL_ENABLE
              value: "1"
            - name: NCCL_CUMEM_ENABLE
              value: "1"
            - name: NCCL_NVLS_ENABLE
              value: "1"
            - name: NCCL_DEBUG
              value: "WARN"
            - name: NCCL_TIMEOUT
              value: "1800"
            - name: TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC
              value: "1800"
            - name: GLOO_SOCKET_IFNAME
              value: eth0
          command:
            - /bin/bash
            - -c
          args:
            - |-
              python3 -m dynamo.vllm \
                --model /model-cache/deepseek-r1-fp4 \
                --served-model-name deepseek-ai/DeepSeek-R1 \
                --data-parallel-size 8 \
                --kv-cache-dtype fp8 \
                --tensor-parallel-size 1 \
                --pipeline-parallel-size 1 \
                --enable-expert-parallel \
                --max-model-len 4096 \
                --no-enable-prefix-caching \
                --trust-remote-code \
                --no-enable-chunked-prefill \
                --all2all-backend allgather_reducescatter \
                --data-parallel-hybrid-lb \
                --compilation_config.custom_ops+=+quant_fp8,+rms_norm,+rotary_embedding \
                --compilation_config.pass_config.fuse_attn_quant true \
                --compilation_config.pass_config.fuse_allreduce_rms true \
                --compilation_config.pass_config.eliminate_noops true \
                --async-scheduling \
                --compilation-config '{"cudagraph_mode":"FULL_DECODE_ONLY"}' \
                --gpu-memory-utilization 0.9 \
                --stream-interval 50 \
                --max-num-seqs 512 \
                --max-num-batched-tokens 4096 \
                --max-cudagraph-capture-size 512
    prefill:
      componentType: worker
      subComponentType: prefill
      replicas: 1
      multinode:
        nodeCount: 2
      resources:
        requests:
          cpu: "130"
          memory: "850Gi"
        limits:
          cpu: "130"
          memory: "850Gi"
          gpu: "4"
        claims:
          - name: compute-domain-channel
      volumeMounts:
        - name: model-cache
          mountPoint: /model-cache
      sharedMemory:
        size: 800Gi
      extraPodSpec:
        containers: []
        imagePullSecrets:
          - name: nvcr-pull-secret
        resourceClaims:
          - name: compute-domain-channel
            resourceClaimTemplateName: vllm-dsr1-gb200-compute-domain-channel
        tolerations:
          - key: "dedicated"
            operator: "Equal"
            value: "user-workload"
            effect: "NoSchedule"
          - key: "dedicated"
            operator: "Equal"
            value: "user-workload"
            effect: "NoExecute"
        mainContainer:
          startupProbe:
            httpGet:
              path: /health
              port: 9090
            periodSeconds: 10
            timeoutSeconds: 10
            failureThreshold: 600
          image: nvcr.io/nvidian/dynamo-dev/vllm-runtime:ptarasiewicz-dsr1-gb200-arm64
          workingDir: /workspace/dynamo
          env:
            # --- Blackwell FP4 kernel optimizations ---
            - name: VLLM_USE_FLASHINFER_MOE_FP4
              value: "1"
            - name: VLLM_USE_TRTLLM_RAGGED_DEEPSEEK_PREFILL
              value: "1"
            - name: VLLM_USE_NCCL_SYMM_MEM
              value: "1"
            # --- MoE / Expert Parallel ---
            - name: VLLM_SKIP_P2P_CHECK
              value: "1"
            # --- NVLink-C2C / Multi-node NVLink ---
            - name: NCCL_MNNVL_ENABLE
              value: "1"
            - name: NCCL_CUMEM_ENABLE
              value: "1"
            - name: NCCL_NVLS_ENABLE
              value: "1"
            - name: NCCL_DEBUG
              value: "WARN"
            - name: NCCL_TIMEOUT
              value: "1800"
            - name: TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC
              value: "1800"
            - name: GLOO_SOCKET_IFNAME
              value: eth0
          command:
            - /bin/bash
            - -c
          args:
            - |-
              python3 -m dynamo.vllm \
                --model /model-cache/deepseek-r1-fp4 \
                --is-prefill-worker \
                --served-model-name deepseek-ai/DeepSeek-R1 \
                --data-parallel-size 8 \
                --kv-cache-dtype fp8 \
                --tensor-parallel-size 1 \
                --pipeline-parallel-size 1 \
                --enable-expert-parallel \
                --max-model-len 4096 \
                --no-enable-prefix-caching \
                --trust-remote-code \
                --no-enable-chunked-prefill \
                --all2all-backend allgather_reducescatter \
                --data-parallel-hybrid-lb \
                --compilation_config.custom_ops+=+quant_fp8,+rms_norm,+rotary_embedding \
                --compilation_config.pass_config.fuse_attn_quant true \
                --compilation_config.pass_config.fuse_allreduce_rms true \
                --compilation_config.pass_config.eliminate_noops true \
                --async-scheduling \
                --swap-space 16 \
                --max-num-seqs 8 \
                --enforce-eager \
                --gpu-memory-utilization 0.9 \
                --max-num-batched-tokens 16384
