apiVersion: batch/v1
kind: Job
metadata:
  name: oss-gpt120b-agg-perf
spec:
  backoffLimit: 3
  completions: 1
  parallelism: 1
  template:
    metadata:
      labels:
        app: oss-gpt120b-agg-perf
    spec:
      restartPolicy: Never
      tolerations:
        - key: "dedicated"
          operator: "Equal"
          value: "user-workload"
          effect: "NoSchedule"
        - key: "dedicated"
          operator: "Equal"
          value: "user-workload"
          effect: "NoExecute"
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: nodeGroup
                    operator: In
                    values:
                      - "customer-cpu"
      containers:
      - name: perf
        image: nvcr.io/nvidian/nim-llm-dev/vllm-runtime:aiperf-0637181
        workingDir: /workspace/components/backends/vllm
        command:
        - /bin/sh
        - -c
        - |
          apt-get update && apt-get install -y curl jq
          # wait for the model to be ready
          export ENDPOINT=gpt-oss-agg-trtllmworker:8000
          export TARGET_MODEL=openai/gpt-oss-120b
          export INTERVAL=5
          echo "Waiting for model '$TARGET_MODEL' at $ENDPOINT/v1/models (checking every ${INTERVAL}s)..."
          while ! curl -s "http://$ENDPOINT/v1/models" | jq -e --arg model "$TARGET_MODEL" '.data[]? | select(.id == $model)' >/dev/null 2>&1; do
              echo "[$(date '+%H:%M:%S')] Model not ready yet, waiting ${INTERVAL}s..."
              sleep $INTERVAL
          done
          echo "âœ… Model '$TARGET_MODEL' is now available!"
          curl -s "http://$ENDPOINT/v1/models" | jq .
          # now run the benchmark
          export ARTIFACT_DIR="/tmp/genai"
          mkdir -p "$ARTIFACT_DIR"
          echo "Running benchmark..."
          export COLUMNS=200
          export x=2048
          export y=1000000
          aiperf profile \
              --model $TARGET_MODEL \
              --tokenizer ~/.cache/huggingface/hub/models--openai--gpt-oss-120b/snapshots/b5c939de8f754692c1647ca79fbf85e8c1e70f8a  \
              --endpoint-type chat \
              --endpoint /v1/chat/completions \
              --streaming \
              --url http://$ENDPOINT \
              --synthetic-input-tokens-mean 1000 \
              --synthetic-input-tokens-stddev 0 \
              --output-tokens-mean 2000 \
              --output-tokens-stddev 0 \
              --extra-inputs "{\"max_tokens\":2000}" \
              --extra-inputs "{\"min_tokens\":2000}" \
              --extra-inputs "{\"ignore_eos\":true}" \
              --extra-inputs "{\"nvext\":{\"ignore_eos\":true}}" \
              --concurrency $x \
              --request-count $((3*x)) \
              --warmup-request-count $x \
              --conversation-num 1 \
              --random-seed 100 \
              --request-rate $y --workers-max 128 \
              -H 'Authorization: Bearer NOT USED' \
              -H 'Accept: text/event-stream' --record-processors 32 --ui simple
          echo "Benchmark completed successfully!"
        volumeMounts:
        - name: model-cache
          mountPath: /root/.cache/huggingface
      imagePullSecrets:
        - name: nvcrimagepullsecret
      volumes:
      - name: model-cache
        persistentVolumeClaim:
          claimName: model-cache-oss-gpt120b
