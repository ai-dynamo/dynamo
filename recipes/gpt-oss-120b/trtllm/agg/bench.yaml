# SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0
apiVersion: batch/v1
kind: Job
metadata:
  name: oss-gpt120b-bench
spec:
  backoffLimit: 1
  completions: 1
  parallelism: 1
  template:
    metadata:
      labels:
        app: oss-gpt120b-bench
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: nvidia.com/dynamo-graph-deployment-name
                    operator: In
                    values:
                      - gpt-oss-agg
              topologyKey: kubernetes.io/hostname
      containers:
      - command:
        - /bin/sh
        - -c
        - |
          #TODO: this can be baked into the aiperf image
          apt-get update && apt-get install -y curl jq procps
          sysctl -w net.ipv4.ip_local_port_range="1024 65000"
          cat /proc/sys/net/ipv4/ip_local_port_range
          export COLUMNS=200
          EPOCH=$(date +%s)
          ## utility functions -- can be moved to a bash script / configmap
          wait_for_model_ready() {
            echo "Waiting for model '$TARGET_MODEL' at $ENDPOINT/v1/models (checking every 5s)..."
            while ! curl -s "http://$ENDPOINT/v1/models" | jq -e --arg model "$TARGET_MODEL" '.data[]? | select(.id == $model)' >/dev/null 2>&1; do
                echo "[$(date '+%H:%M:%S')] Model not ready yet, waiting 5s..."
                sleep 5
            done
            echo "âœ… Model '$TARGET_MODEL' is now available!"
            echo "Model '$TARGET_MODEL' is now available!"
            curl -s "http://$ENDPOINT/v1/models" | jq .
          }
          run_perf() {
            local concurrency=$1
            local isl=$2
            local osl=$3
            key=concurrency_${concurrency}
            export ARTIFACT_DIR="${ROOT_ARTIFACT_DIR}/${EPOCH}_${JOB_NAME}/${key}"
            mkdir -p "$ARTIFACT_DIR"
            aiperf profile --artifact-dir $ARTIFACT_DIR \
                --model $TARGET_MODEL \
                --tokenizer ~/.cache/huggingface/hub/models--openai--gpt-oss-120b/snapshots/b5c939de8f754692c1647ca79fbf85e8c1e70f8a  \
                --endpoint-type chat \
                --endpoint /v1/chat/completions \
                --streaming \
                --url http://$ENDPOINT \
                --synthetic-input-tokens-mean $isl \
                --synthetic-input-tokens-stddev 0 \
                --output-tokens-mean $osl \
                --output-tokens-stddev 0 \
                --extra-inputs "{\"max_tokens\":$osl}" \
                --extra-inputs "{\"min_tokens\":$osl}" \
                --extra-inputs "{\"ignore_eos\":true}" \
                --extra-inputs "{\"nvext\":{\"ignore_eos\":true}}" \
                --concurrency $concurrency \
                --request-count $((2*concurrency)) \
                --warmup-request-count $concurrency \
                --conversation-num 1 \
                --random-seed 100 \
                --request-rate 100000 \
                --workers-max 252 \
                -H 'Authorization: Bearer NOT USED' \
                -H 'Accept: text/event-stream'\
                --record-processors 32 \
                --ui simple
            echo "ARTIFACT_DIR: $ARTIFACT_DIR"
            ls -la $ARTIFACT_DIR
          }
          #### Actual execution ####
          wait_for_model_ready
          mkdir -p "${ROOT_ARTIFACT_DIR}/${EPOCH}_${JOB_NAME}"
          # Write input_config.json
          cat > "${ROOT_ARTIFACT_DIR}/${EPOCH}_${JOB_NAME}/input_config.json" <<EOF
          {
            "gpu_count": $DEPLOYMENT_GPU_COUNT,
            "mode": "$DEPLOYMENT_MODE",
            "isl": $ISL,
            "osl": $OSL,
            "endpoint": "$ENDPOINT",
            "model endpoint": "$TARGET_MODEL"
          }
          EOF
          # Run perf for each concurrency
          for concurrency in $CONCURRENCIES; do
            run_perf $concurrency $ISL $OSL
            sleep 10
          done
        env:
        - name: TARGET_MODEL
          value: openai/gpt-oss-120b
        - name: ENDPOINT
          value: gpt-oss-agg-frontend:8000
        - name: CONCURRENCIES
          value: "130000"
        - name: ISL
          value: "16"
        - name: OSL
          value: "1000"
        - name: DEPLOYMENT_MODE
          value: agg
        - name: DEPLOYMENT_GPU_COUNT
          value: "72"
        - name: AIPERF_HTTP_CONNECTION_LIMIT
          value: "252"
        - name: JOB_NAME
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.labels['job-name']
        - name: ROOT_ARTIFACT_DIR
          value: /root/.cache/huggingface/hub/perf
        image: nvcr.io/nvidian/nim-llm-dev/vllm-runtime:aiperf-lim-2-31b447d6
        imagePullPolicy: IfNotPresent
        name: perf
        resources: {}
        securityContext:
          privileged: true
        volumeMounts:
        - name: model-cache
          mountPath: /root/.cache/huggingface/hub
        workingDir: /workspace/components/backends/vllm
      dnsPolicy: ClusterFirst
      imagePullSecrets:
      - name: nvcrimagepullsecret
      restartPolicy: Never
      schedulerName: default-scheduler
      volumes:
      - name: model-cache
        persistentVolumeClaim:
          claimName: model-cache
