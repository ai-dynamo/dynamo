# SPDX-FileCopyrightText: Copyright (c) 2025-2026 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0
apiVersion: nvidia.com/v1alpha1
kind: DynamoGraphDeployment
metadata:
  name: llama3-70b-agg
spec:
  backendFramework: vllm
  pvcs:
    - name: model-cache
      create: false
  services:
    Epp:
      envFromSecret: hf-token-secret
      componentType: epp
      replicas: 1
      extraPodSpec:
        mainContainer:
          image: nvcr.io/nvidia/ai-dynamo/frontend:my-tag
      eppConfig:
        # This config uses the same disagg-profile-handler as disaggregated deployments.
        # The handler's graceful degradation feature makes this possible:
        # - With no "prefill" profile defined, it runs only the "decode" profile.
        # - The decode scorer receives isDisaggregated=false, so the Dynamo KV router
        #   uses full overlap scoring (overlap_score_weight=1.0) for aggregated mode.
        # - If prefill workers were added later (and a prefill profile configured),
        #   the same handler would automatically switch to disaggregated routing.
        config:
          plugins:
            - type: disagg-profile-handler
            - name: decode-filter
              type: label-filter
              # allowsNoLabel: true lets pods without the subComponentType label pass through,
              # which is typical for aggregated deployments where workers don't have this label.
              parameters:
                label: "nvidia.com/dynamo-sub-component-type"
                validValues:
                  - "decode"
                allowsNoLabel: true
            - name: picker
              type: max-score-picker
            - name: dyn-decode
              type: dyn-decode-scorer
          # Only a "decode" profile â€” no "prefill" profile means pure aggregated mode.
          schedulingProfiles:
            - name: decode
              plugins:
                - pluginRef: decode-filter
                - pluginRef: dyn-decode
                  weight: 1
                - pluginRef: picker
    VllmDecodeWorker:
      componentType: worker
      envFromSecret: hf-token-secret
      volumeMounts:
        - name: model-cache
          mountPoint: /opt/models
      sharedMemory:
        size: 20Gi
      extraPodSpec:
        mainContainer:
          env:
            - name: SERVED_MODEL_NAME
              value: "RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic"
            - name: MODEL_PATH
              value: "RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic"
            - name: HF_HOME
              value: /opt/models
            - name: DYN_STORE_KV
              value: "mem"
          args:
          - "python3 -m dynamo.vllm --model $MODEL_PATH --served-model-name $SERVED_MODEL_NAME --tensor-parallel-size 4 --data-parallel-size 1 --gpu-memory-utilization 0.90 --no-enable-prefix-caching --block-size 128"
          command:
          - /bin/sh
          - -c
          image: nvcr.io/nvidia/ai-dynamo/vllm-runtime:my-tag
          workingDir: /workspace/examples/backends/vllm
              # Frontend sidecar: receives requests from kGateway on port 8000
        # and routes them to the vLLM worker in the same pod
        containers:
          - name: frontend
            image: nvcr.io/nvidia/ai-dynamo/vllm-runtime:my-tag
            command:
              - python3
            args:
              - -m
              - dynamo.frontend
              - --router-mode
              - direct
            ports:
              - containerPort: 8000
                name: http
                protocol: TCP
            envFrom:
              - secretRef:
                  name: hf-token-secret
            env:
              - name: DYNAMO_PORT
                value: "8000"
              - name: DYN_HTTP_PORT
                value: "8000"
              - name: DYN_NAMESPACE
                value: my-model-vllm-agg
              - name: DYN_COMPONENT
                value: frontend
              - name: DYN_DISCOVERY_BACKEND
                value: kubernetes
              - name: DYN_PARENT_DGD_K8S_NAME
                value: llama3-70b-agg
              - name: DYN_PARENT_DGD_K8S_NAMESPACE
                value: my-model
              - name: POD_NAME
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.name
              - name: POD_NAMESPACE
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.namespace
              - name: POD_UID
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.uid
            livenessProbe:
              httpGet:
                path: /live
                port: http
              initialDelaySeconds: 15
              periodSeconds: 10
            readinessProbe:
              httpGet:
                path: /health
                port: http
              initialDelaySeconds: 10
              periodSeconds: 10
      replicas: 1
      resources:
        limits:
          gpu: "4"
        requests:
          gpu: "4"
