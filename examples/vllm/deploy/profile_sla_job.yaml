apiVersion: batch/v1
kind: Job
metadata:
  name: profile-sla
  namespace: ${NAMESPACE}
spec:
  template:
    spec:
      serviceAccountName: profile-sla-sa
      # Add node affinity to avoid problematic nodes with GPU/NVLink issues
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: kubernetes.io/hostname
                operator: NotIn
                values:
                - aks-agentpool-36388387-vmss000003  # NVLink communication failure
                - aks-agentpool-36388387-vmss000005  # NotReady status
      containers:
      - name: profile-sla
        image: nvcr.io/nvidian/nim-llm-dev/vllm_v1-runtime:dep-233.8
        resources:
          requests:
            cpu: "1"
            memory: "2Gi"
          limits:
            cpu: "2"
            memory: "4Gi"
        env:
          - name: HUGGING_FACE_HUB_TOKEN
            valueFrom:
              secretKeyRef:
                name: hf-token-secret
                key: HF_TOKEN
          - name: NATS_SERVER
            value: nats://${NAMESPACE}-nats:4222
          - name: ETCD_ENDPOINTS
            value: ${NAMESPACE}-etcd:2379
        command: ["python", "/workspace/benchmarks/profiler/profile_sla.py"]
        args:
          - --config
          - /workspace/examples/vllm/deploy/disagg.yaml
          - --output-dir
          - /workspace/profiling_results
          - --namespace
          - ${NAMESPACE}
        volumeMounts:
          - name: output-volume
            mountPath: /workspace/profiling_results
      restartPolicy: Never
      volumes:
        - name: output-volume
          persistentVolumeClaim:
            claimName: profiling-pvc
  backoffLimit: 1
