# SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0
apiVersion: v1
kind: ConfigMap
metadata:
  name: nvidia-config
data:
  prefill.yaml: |
    # SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
    # SPDX-License-Identifier: Apache-2.0
    #
    # Licensed under the Apache License, Version 2.0 (the "License");
    # you may not use this file except in compliance with the License.
    # You may obtain a copy of the License at
    #
    # http://www.apache.org/licenses/LICENSE-2.0
    #
    # Unless required by applicable law or agreed to in writing, software
    # distributed under the License is distributed on an "AS IS" BASIS,
    # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    # See the License for the specific language governing permissions and
    # limitations under the License.
    tensor_parallel_size: 8
    moe_expert_parallel_size: 1
    enable_attention_dp: false
    max_num_tokens: 8192
    trust_remote_code: true
    backend: pytorch
    enable_chunked_prefill: true
    # Overlap scheduler not currently supported in prefill only workers.
    disable_overlap_scheduler: true
    kv_cache_config:
      free_gpu_memory_fraction: 0.80
    cache_transceiver_config:
      backend: DEFAULT

  decode.yaml: |
    # SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
    # SPDX-License-Identifier: Apache-2.0
    #
    # Licensed under the Apache License, Version 2.0 (the "License");
    # you may not use this file except in compliance with the License.
    # You may obtain a copy of the License at
    #
    # http://www.apache.org/licenses/LICENSE-2.0
    #
    # Unless required by applicable law or agreed to in writing, software
    # distributed under the License is distributed on an "AS IS" BASIS,
    # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    # See the License for the specific language governing permissions and
    # limitations under the License.
    tensor_parallel_size: 8
    moe_expert_parallel_size: 1
    enable_attention_dp: false
    max_num_tokens: 8192
    trust_remote_code: true
    backend: pytorch
    enable_chunked_prefill: true
    disable_overlap_scheduler: false
    kv_cache_config:
      free_gpu_memory_fraction: 0.80
    cache_transceiver_config:
      backend: DEFAULT
---
apiVersion: nvidia.com/v1alpha1
kind: DynamoGraphDeployment
metadata:
  name: trtllm-disagg-tp8
spec:
  backendFramework: trtllm
  pvcs:
    - name: model-cache-pvc
      create: false
  envs:
    - name: OMPI_ALLOW_RUN_AS_ROOT
      value: "1"
    - name: OMPI_ALLOW_RUN_AS_ROOT_CONFIRM
      value: "1"
  services:
    Frontend:
      dynamoNamespace: trtllm-disagg
      componentType: frontend
      replicas: 1
      extraPodSpec:
        mainContainer:
          image: my-registry/trtllm-runtime:my-tag
          workingDir: /workspace/examples/backends/trtllm
          command:
          - python3
          - -m
          - dynamo.frontend
          args:
            - --http-port
            - "8000"
    ModelExpress:
      envFromSecret: hf-token-secret
      dynamoNamespace: trtllm-disagg
      componentType: frontend
      readinessProbe:
        tcpSocket:
          port: 8000
        initialDelaySeconds: 10
        periodSeconds: 5
        timeoutSeconds: 3
        failureThreshold: 3
      replicas: 1
      resources:
        requests:
          cpu: "4"
          memory: "16Gi"
        limits:
          cpu: "4"
          memory: "16Gi"
      extraPodSpec:
        mainContainer:
          image: nvcr.io/nvidia/ai-dynamo/modelexpress-server:my-tag
          imagePullPolicy: IfNotPresent
          env:
            - name: MODEL_EXPRESS_SERVER_PORT
              value: "8000"
            - name: MODEL_EXPRESS_LOGGING_LEVEL
              value: "info"
            - name: MODEL_EXPRESS_DATABASE_PATH
              value: "/model/models.db"
            - name: MODEL_EXPRESS_CACHE_DIRECTORY
              value: "/model/.model-express/cache"
            - name: HF_HUB_CACHE
              value: "/model/.model-express/cache"
          command:
            - /bin/sh
            - -c
          args:
            - |
              echo "Setting up Model Express configuration..."

              mkdir -p $MODEL_EXPRESS_CACHE_DIRECTORY
              cat > $MODEL_EXPRESS_CACHE_DIRECTORY/config.yaml << EOF
              local_path: $MODEL_EXPRESS_CACHE_DIRECTORY
              server_endpoint: http://localhost:8000
              timeout_secs: null
              EOF

              ./modelexpress-server &

              SERVER_PID=$!
              echo "Server started with PID: $SERVER_PID"
              wait $SERVER_PID
      volumeMounts:
        - name: model-cache-pvc
          mountPoint: /model
    prefill:
      volumeMounts:
        - name: model-cache-pvc
          mountPoint: /model
      dynamoNamespace: trtllm-disagg
      envFromSecret: hf-token-secret
      componentType: worker
      replicas: 1
      multinode:
        nodeCount: 2
      resources:
        limits:
          gpu: "4"
      extraPodSpec:
        volumes:
          - name: nvidia-config
            configMap:
              name: nvidia-config
        mainContainer:
          env:
            - name: MODEL_EXPRESS_URL
              value: http://trtllm-disagg-modelexpress:8000
            - name: MODEL_EXPRESS_CACHE_DIRECTORY
              value: "/model/.model-express/cache"
            - name: HF_HUB_CACHE
              value: "/model/.model-express/cache"
          volumeMounts:
            - name: nvidia-config
              mountPath: /workspace/
              readOnly: true
          image: my-registry/trtllm-runtime:my-tag
          workingDir: /workspace/
          command:
          - python3
          - -m
          - dynamo.trtllm
          args:
            - --model-path
            - Qwen/Qwen3-0.6B
            - --served-model-name
            - Qwen/Qwen3-0.6B
            - --extra-engine-args
            - /workspace/prefill.yaml
            - --disaggregation-mode
            - prefill
            - --disaggregation-strategy
            - decode_first
    decode:
      volumeMounts:
        - name: model-cache-pvc
          mountPoint: /model
      dynamoNamespace: trtllm-disagg
      envFromSecret: hf-token-secret
      componentType: worker
      replicas: 1
      multinode:
        nodeCount: 2
      resources:
        limits:
          gpu: "4"
      extraPodSpec:
        volumes:
          - name: nvidia-config
            configMap:
              name: nvidia-config
        mainContainer:
          env:
            - name: MODEL_EXPRESS_URL
              value: http://trtllm-disagg-modelexpress:8000
            - name: MODEL_EXPRESS_CACHE_DIRECTORY
              value: "/model/.model-express/cache"
            - name: HF_HUB_CACHE
              value: "/model/.model-express/cache"
          volumeMounts:
            - name: nvidia-config
              mountPath: /workspace/
              readOnly: true
          image: my-registry/trtllm-runtime:my-tag
          workingDir: /workspace/
          command:
          - python3
          - -m
          - dynamo.trtllm
          args:
            - --model-path
            - Qwen/Qwen3-0.6B
            - --served-model-name
            - Qwen/Qwen3-0.6B
            - --extra-engine-args
            - /workspace/decode.yaml
            - --disaggregation-mode
            - decode
            - --disaggregation-strategy
            - decode_first
