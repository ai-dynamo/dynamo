# SPDX-FileCopyrightText: Copyright (c) 2025-2026 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0

# E2E test for the full GlobalPlanner flow with --throughput-metrics-source=router.
#
# This validates the complete pipeline:
#   1. GlobalRouter forwards requests to pool LocalRouters
#   2. LocalRouter pods emit dynamo_component_router_* metrics to cluster Prometheus
#   3. SLA Planner reads router histogram metrics (no dynamo_namespace filter — our bug fix)
#      and computes scaling decisions
#   4. SLA Planner sends scaling decisions to GlobalPlanner via the scale_request endpoint
#   5. GlobalPlanner receives and logs the requests in NO-OPERATION mode
#      (does not call K8s API — prevents local planner from crashing on scaling errors)
#
# Architecture:
#   DGD 1 (gp-ctrl):    Frontend + GlobalRouter + GlobalPlanner (no-op) + SLA Planner
#                        Dynamo namespace: darfeen-dynamo-cloud-gp-ctrl
#   DGD 2 (gp-prefill): LocalRouter (port 9090) + MockerPrefill
#                        Dynamo namespace: darfeen-dynamo-cloud-gp-prefill
#   DGD 3 (gp-decode):  LocalRouter (port 9090) + MockerDecode
#                        Dynamo namespace: darfeen-dynamo-cloud-gp-decode
#
# Both GlobalPlanner and SLA Planner self-patch from feat/throughput-metrics-source:
#   - GlobalPlanner gets: scale_handler.py (no-op mode), argparse_config.py, __main__.py
#   - SLA Planner gets:   entire planner/ directory (new __main__.py + PlannerConfig),
#                         prometheus_names.py (router metric constants)
#
# The SLA Planner runs with environment=global-planner and throughput_metrics_source=router.
# It reads aggregate router histograms from Prometheus (all routers, no namespace filter)
# and sends TargetReplica scale requests to GlobalPlanner.GlobalPlanner logs [NO-OP] lines
# and returns ScaleStatus.SUCCESS — local planner never crashes.
#
# Prerequisites (already deployed in darfeen-dynamo-cloud):
#   kubectl apply -f standalone-prometheus.yaml -n darfeen-dynamo-cloud
#
# Optional: tear down the existing mocker-hierarchical-* DGDs first to free resources
#   kubectl delete dgd mocker-hierarchical-frontend mocker-hierarchical-prefill-pool \
#     mocker-hierarchical-decode-pool -n darfeen-dynamo-cloud
#
# Deploy:
#   kubectl apply -f aks-router-metrics-test-global-planner.yaml -n darfeen-dynamo-cloud
#
# Watch GlobalPlanner receive scale requests:
#   kubectl logs -n darfeen-dynamo-cloud -l nvidia.com/dynamo-component=GlobalPlanner -f \
#     | grep -E "NO-OP|Scale request|Starting GlobalPlanner"
#
# Watch SLA Planner compute metrics and delegate to GlobalPlanner:
#   kubectl logs -n darfeen-dynamo-cloud -l nvidia.com/dynamo-component-type=planner -f \
#     | grep -E "adjustment started|Observed|Delegating scale|GlobalPlanner scaling"
#
# Send load:
#   ./send_load.sh darfeen-dynamo-cloud
#
# Cleanup:
#   kubectl delete -f aks-router-metrics-test-global-planner.yaml -n darfeen-dynamo-cloud

# =============================================================================
# GlobalRouter ConfigMap
# Pool namespaces = "{k8s-namespace}-{dgd-metadata-name}"
# =============================================================================
apiVersion: v1
kind: ConfigMap
metadata:
  name: gp-global-router-config
data:
  global_router_config.json: |
    {
      "num_prefill_pools": 1,
      "num_decode_pools": 1,
      "prefill_pool_dynamo_namespaces": [
        "darfeen-dynamo-cloud-gp-prefill"
      ],
      "decode_pool_dynamo_namespaces": [
        "darfeen-dynamo-cloud-gp-decode"
      ],
      "prefill_pool_selection_strategy": {
        "ttft_min": 10, "ttft_max": 3000, "ttft_resolution": 2,
        "isl_min": 0,   "isl_max": 32000, "isl_resolution": 2,
        "prefill_pool_mapping": [[0,0],[0,0]]
      },
      "decode_pool_selection_strategy": {
        "itl_min": 10,  "itl_max": 500,   "itl_resolution": 2,
        "context_length_min": 0, "context_length_max": 32000, "context_length_resolution": 2,
        "decode_pool_mapping": [[0,0],[0,0]]
      }
    }
---
# =============================================================================
# DGD 1: Frontend + GlobalRouter + GlobalPlanner (no-op) + SLA Planner
# Dynamo namespace: darfeen-dynamo-cloud-gp-ctrl
# =============================================================================
apiVersion: nvidia.com/v1alpha1
kind: DynamoGraphDeployment
metadata:
  name: gp-ctrl
spec:
  services:
    Frontend:
      componentType: frontend
      replicas: 1
      extraPodSpec:
        imagePullSecrets:
          - name: docker-imagepullsecret
        mainContainer:
          image: dynamoci.azurecr.io/ai-dynamo/dynamo:451e3d9ea6b1a4bceb55f6a798fd857a59dfd319-vllm-cuda12-amd64
          workingDir: /workspace
          command:
            - python3
            - -m
            - dynamo.frontend
          args:
            - --router-mode
            - round-robin
            - --namespace
            - darfeen-dynamo-cloud-gp-ctrl
            - --model-name
            - nvidia/Llama-3.1-8B-Instruct-FP8

    GlobalRouter:
      componentType: default
      replicas: 1
      extraPodSpec:
        imagePullSecrets:
          - name: docker-imagepullsecret
        volumes:
          - name: global-router-config
            configMap:
              name: gp-global-router-config
        mainContainer:
          image: dynamoci.azurecr.io/ai-dynamo/dynamo:451e3d9ea6b1a4bceb55f6a798fd857a59dfd319-vllm-cuda12-amd64
          workingDir: /workspace
          command:
            - python3
            - -m
            - dynamo.global_router
          args:
            - --config
            - /config/global_router_config.json
            - --model-name
            - nvidia/Llama-3.1-8B-Instruct-FP8
            - --namespace
            - darfeen-dynamo-cloud-gp-ctrl
          volumeMounts:
            - name: global-router-config
              mountPath: /config
              readOnly: true

    GlobalPlanner:
      componentType: default
      replicas: 1
      extraPodSpec:
        imagePullSecrets:
          - name: docker-imagepullsecret
        mainContainer:
          image: dynamoci.azurecr.io/ai-dynamo/dynamo:451e3d9ea6b1a4bceb55f6a798fd857a59dfd319-vllm-cuda12-amd64
          command:
            - bash
            - -c
            - |
              set -e
              SP=/opt/dynamo/venv/lib/python3.12/site-packages
              echo "Cloning feat/throughput-metrics-source for GlobalPlanner patch..."
              git clone --depth=1 \
                --branch feat/throughput-metrics-source \
                https://github.com/ai-dynamo/dynamo /tmp/dynamo-pr 2>&1 | tail -3
              SRC_COMP=/tmp/dynamo-pr/components/src/dynamo
              echo "Patching venv (global_planner)..."
              cp $SRC_COMP/global_planner/argparse_config.py  $SP/dynamo/global_planner/argparse_config.py
              cp $SRC_COMP/global_planner/scale_handler.py    $SP/dynamo/global_planner/scale_handler.py
              cp $SRC_COMP/global_planner/__main__.py         $SP/dynamo/global_planner/__main__.py
              echo "Patches applied. Starting GlobalPlanner in no-op mode..."
              exec python3 -m dynamo.global_planner --no-operation

    Planner:
      componentType: planner
      replicas: 1
      extraPodSpec:
        imagePullSecrets:
          - name: docker-imagepullsecret
        mainContainer:
          image: dynamoci.azurecr.io/ai-dynamo/dynamo:451e3d9ea6b1a4bceb55f6a798fd857a59dfd319-vllm-cuda12-amd64
          command:
            - bash
            - -c
            - |
              set -e
              SP=/opt/dynamo/venv/lib/python3.12/site-packages
              echo "Cloning feat/throughput-metrics-source for SLA Planner patch..."
              git clone --depth=1 \
                --branch feat/throughput-metrics-source \
                https://github.com/ai-dynamo/dynamo /tmp/dynamo-pr 2>&1 | tail -3
              SRC_LIB=/tmp/dynamo-pr/lib/bindings/python/src/dynamo
              SRC_COMP=/tmp/dynamo-pr/components/src/dynamo
              echo "Patching venv (planner)..."
              cp -r $SRC_COMP/planner $SP/dynamo/
              cp $SRC_LIB/prometheus_names.py $SP/dynamo/prometheus_names.py
              echo "Patches applied. Starting SLA Planner in global-planner mode..."
              exec python3 -m dynamo.planner --config \
                '{"environment":"global-planner","global_planner_namespace":"darfeen-dynamo-cloud-gp-ctrl","backend":"mocker","mode":"disagg","throughput_metrics_source":"router","throughput_adjustment_interval":30,"ttft":2000,"itl":200,"max_gpu_budget":-1,"prefill_engine_num_gpu":1,"decode_engine_num_gpu":1,"no_correction":true,"profile_results_dir":"/workspace/tests/planner/profiling_results/H200_TP1P_TP1D","metric_pulling_prometheus_endpoint":"http://prometheus:9090","model_name":"nvidia/Llama-3.1-8B-Instruct-FP8","enable_load_scaling":false}'
---
# =============================================================================
# DGD 2: Prefill Pool
# Dynamo namespace: darfeen-dynamo-cloud-gp-prefill
# LocalRouter exposes dynamo_component_router_* on DYN_SYSTEM_PORT=9090
# =============================================================================
apiVersion: nvidia.com/v1alpha1
kind: DynamoGraphDeployment
metadata:
  name: gp-prefill
spec:
  services:
    LocalRouter:
      componentType: default
      replicas: 1
      extraPodSpec:
        imagePullSecrets:
          - name: docker-imagepullsecret
        mainContainer:
          image: dynamoci.azurecr.io/ai-dynamo/dynamo:451e3d9ea6b1a4bceb55f6a798fd857a59dfd319-vllm-cuda12-amd64
          workingDir: /workspace
          env:
            # Expose system_status_server so Prometheus can scrape router metrics
            - name: DYN_SYSTEM_PORT
              value: "9090"
          command:
            - python3
            - -m
            - dynamo.router
          args:
            - --endpoint
            - darfeen-dynamo-cloud-gp-prefill.prefill.generate
            - --no-router-kv-events

    MockerPrefill:
      componentType: worker
      subComponentType: prefill
      replicas: 1
      extraPodSpec:
        imagePullSecrets:
          - name: docker-imagepullsecret
        mainContainer:
          image: dynamoci.azurecr.io/ai-dynamo/dynamo:451e3d9ea6b1a4bceb55f6a798fd857a59dfd319-vllm-cuda12-amd64
          workingDir: /workspace
          command:
            - python3
            - -m
            - dynamo.mocker
          args:
            - --model-path
            - nvidia/Llama-3.1-8B-Instruct-FP8
            - --model-name
            - nvidia/Llama-3.1-8B-Instruct-FP8
            - --speedup-ratio
            - "5.0"
            - --planner-profile-data
            - /workspace/tests/planner/profiling_results/H200_TP1P_TP1D
            - --is-prefill-worker
---
# =============================================================================
# DGD 3: Decode Pool
# Dynamo namespace: darfeen-dynamo-cloud-gp-decode
# LocalRouter exposes dynamo_component_router_* on DYN_SYSTEM_PORT=9090
# =============================================================================
apiVersion: nvidia.com/v1alpha1
kind: DynamoGraphDeployment
metadata:
  name: gp-decode
spec:
  services:
    LocalRouter:
      componentType: default
      replicas: 1
      extraPodSpec:
        imagePullSecrets:
          - name: docker-imagepullsecret
        mainContainer:
          image: dynamoci.azurecr.io/ai-dynamo/dynamo:451e3d9ea6b1a4bceb55f6a798fd857a59dfd319-vllm-cuda12-amd64
          workingDir: /workspace
          env:
            # Expose system_status_server so Prometheus can scrape router metrics
            - name: DYN_SYSTEM_PORT
              value: "9090"
          command:
            - python3
            - -m
            - dynamo.router
          args:
            - --endpoint
            - darfeen-dynamo-cloud-gp-decode.backend.generate
            - --no-router-kv-events
            - --router-kv-overlap-score-weight=0

    MockerDecode:
      componentType: worker
      subComponentType: decode
      replicas: 1
      extraPodSpec:
        imagePullSecrets:
          - name: docker-imagepullsecret
        mainContainer:
          image: dynamoci.azurecr.io/ai-dynamo/dynamo:451e3d9ea6b1a4bceb55f6a798fd857a59dfd319-vllm-cuda12-amd64
          workingDir: /workspace
          command:
            - python3
            - -m
            - dynamo.mocker
          args:
            - --model-path
            - nvidia/Llama-3.1-8B-Instruct-FP8
            - --model-name
            - nvidia/Llama-3.1-8B-Instruct-FP8
            - --speedup-ratio
            - "5.0"
            - --planner-profile-data
            - /workspace/tests/planner/profiling_results/H200_TP1P_TP1D
