# Dynamo Checkpoint/Restore Infrastructure
# This chart deploys the checkpoint storage (PVC) and CRIU agent (DaemonSet)
# in a single namespace. Install this chart in each namespace where you want
# to enable checkpoint/restore functionality for DynamoGraphDeployments.

# Storage configuration for checkpoints
storage:
  # Storage type: pvc (default), s3, or oci
  type: pvc
  
  # PVC configuration (when type=pvc)
  pvc:
    # Create a new PVC (set to false if using existing PVC)
    create: true
    # PVC name - must match operator configuration
    name: dynamo-checkpoint-pvc
    # PVC size
    size: 100Gi
    # Storage class (leave empty for default)
    storageClass: "csi-mounted-fs-path-sc"
    # Access mode - ReadWriteMany required for multi-pod access
    accessMode: ReadWriteMany
    # Base path for checkpoints (mounted in pods)
    basePath: /checkpoints
  
  # S3 configuration (when type=s3)
  s3:
    # S3 URI (e.g., s3://my-bucket/checkpoints)
    uri: ""
    # Credentials are expected via IRSA or mounted secrets
  
  # OCI configuration (when type=oci)
  oci:
    # OCI URI (e.g., oci://registry.io/repo/checkpoints)
    uri: ""
  
  # Host path for signal files (inter-pod communication)
  signalHostPath: /var/lib/dynamo-checkpoint/signals

# DaemonSet configuration for CRIU checkpoint agent
daemonset:
  # Container image
  image:
    repository: nvcr.io/nvidian/dynamo-dev/schwinns-criu
    tag: criu-agent-cf34e68
    pullPolicy: Always
  
  # Image pull secrets
  imagePullSecrets:
    - name: ngc-secret
  
  # Resource limits and requests
  resources:
    limits:
      cpu: 2
      memory: 4Gi
    requests:
      cpu: 500m
      memory: 1Gi
  
  # Node selector - target GPU nodes
  nodeSelector:
    nvidia.com/gpu.present: "true"
  
  # Tolerations for GPU nodes
  tolerations:
    - key: nvidia.com/gpu
      operator: Exists
      effect: NoSchedule
    - key: dedicated
      operator: Exists
      effect: NoSchedule
  
  # Runtime class name for GPU access
  runtimeClassName: nvidia
  
  # Pod labels
  podLabels: {}
  
  # Pod annotations
  podAnnotations: {}
  
  # Affinity rules
  affinity: {}
  
  # CRIU configuration
  criu:
    # CUDA plugin directory
    cudaPluginDir: /usr/local/lib/criu
    # CRIU timeout in seconds (6 hours)
    timeout: "21600"
    # Ghost file size limit in bytes
    # 512MB is recommended for GPU workloads with large memory allocations
    ghostLimit: "536870912"
  
  # Container runtime socket path
  containerRuntimeSocket: /run/containerd/containerd.sock

# Seccomp profile configuration
seccomp:
  # Deploy seccomp profile for blocking io_uring (required for CRIU)
  deploy: true

# Service account configuration
serviceAccount:
  # Create service account
  create: true
  # Service account name (generated if not set)
  name: ""
  # Annotations for service account (e.g., for IRSA)
  annotations: {}

# RBAC configuration
rbac:
  # Create RBAC resources
  create: true
  
  # Namespace-scoped RBAC (recommended, required for PVC storage)
  # - true (default): Creates Role/RoleBinding, agent watches pods in chart's namespace only
  # - false: Creates ClusterRole/ClusterRoleBinding, agent watches all pods on assigned nodes
  # Note: PVC storage requires namespace-scoped mode (true) as PVCs are namespace-scoped
  namespaceRestricted: true

