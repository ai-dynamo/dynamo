# SPDX-FileCopyrightText: Copyright (c) 2024-2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0

name: GAIE Deployment with Dynamo (vllm)

on:
  workflow_dispatch:
  push:
    branches:
      - main
      - release/*.*.*

concurrency:
  group: ${{ github.workflow }}-${{ github.ref_name || github.run_id }}
  cancel-in-progress: ${{ github.ref != 'refs/heads/main' }}

jobs:
  gaie:
    name: Build Images, Deploy Dynamo, Install GAIE, and Run Script
    runs-on: cpu-amd-m5-2xlarge
    env:
      FRAMEWORK: vllm
      PROFILE: agg
      DYNAMO_INGRESS_SUFFIX: dev.aire.nvidia.com
      DEPLOYMENT_FILE: deploy/${{ env.PROFILE }}.yaml
      MODEL_NAME: Qwen/Qwen3-0.6B
      OPERATOR_IMAGE_TAG: ${{ github.sha }}-operator-amd64
      VLLM_IMAGE_TAG: ${{ github.sha }}-vllm-amd64
      GATEWAY_API_VERSION: v1.3.0
      INFERENCE_EXTENSION_VERSION: v0.5.1
      KGATEWAY_VERSION: v2.0.3
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
        with:
          driver: docker

      - name: Install tooling (curl, jq, yq, helm, kubectl)
        shell: bash
        run: |
          set -euo pipefail
          sudo apt-get update && sudo apt-get install -y curl bash openssl gettext git jq
          curl -L https://github.com/mikefarah/yq/releases/latest/download/yq_linux_amd64 -o yq
          sudo install -m 0755 yq /usr/local/bin/yq
          curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
          sudo chmod 700 get_helm.sh
          sudo ./get_helm.sh
          KUBECTL_VER=$(curl -L -s https://dl.k8s.io/release/stable.txt)
          curl -LO "https://dl.k8s.io/release/${KUBECTL_VER}/bin/linux/amd64/kubectl"
          sudo install -m 0755 kubectl /usr/local/bin/kubectl

      - name: Login to NGC (optional for base images)
        if: github.event.pull_request.head.repo.full_name == github.repository || github.event_name == 'push'
        run: |
          echo "${{ secrets.NGC_CI_ACCESS_TOKEN }}" | docker login nvcr.io -u '$oauthtoken' --password-stdin

      - name: Install awscli (for docker-build caching/login)
        shell: bash
        run: |
          curl "https://awscli.amazonaws.com/awscli-exe-linux-$(uname -m).zip" -o "awscliv2.zip"
          unzip awscliv2.zip
          sudo ./aws/install

      - name: Login to ECR (for docker-build action compatibility)
        shell: bash
        env:
          ECR_HOSTNAME: ${{ secrets.AWS_ACCOUNT_ID }}.dkr.ecr.${{ secrets.AWS_DEFAULT_REGION }}.amazonaws.com
        run: |
          aws ecr get-login-password --region ${{ secrets.AWS_DEFAULT_REGION }} | docker login --username AWS --password-stdin ${ECR_HOSTNAME}

      - name: Build Operator image (amd64)
        id: build-operator
        shell: bash
        env:
          ECR_HOSTNAME: ${{ secrets.AWS_ACCOUNT_ID }}.dkr.ecr.${{ secrets.AWS_DEFAULT_REGION }}.amazonaws.com
        run: |
          set -euo pipefail
          cd deploy/cloud/operator
          docker buildx build --load \
            --platform linux/amd64 \
            --build-arg DOCKER_PROXY=${ECR_HOSTNAME}/dockerhub/ \
            -f Dockerfile \
            -t dynamo-operator:latest .

      - name: Tag and Push Operator image to ACR
        uses: ./.github/actions/docker-tag-push
        with:
          local_image: dynamo-operator:latest
          push_tag: ${{ secrets.AZURE_ACR_HOSTNAME }}/ai-dynamo/dynamo:${{ env.OPERATOR_IMAGE_TAG }}
          aws_push: "false"
          azure_push: "true"
          aws_account_id: ${{ secrets.AWS_ACCOUNT_ID }}
          aws_default_region: ${{ secrets.AWS_DEFAULT_REGION }}
          azure_acr_hostname: ${{ secrets.AZURE_ACR_HOSTNAME }}
          azure_acr_user: ${{ secrets.AZURE_ACR_USER }}
          azure_acr_password: ${{ secrets.AZURE_ACR_PASSWORD }}

      - name: Build vllm runtime image (amd64)
        id: build-vllm
        uses: ./.github/actions/docker-build
        with:
          framework: vllm
          target: runtime
          platform: linux/amd64
          ngc_ci_access_token: ${{ secrets.NGC_CI_ACCESS_TOKEN }}
          ci_token: ${{ secrets.CI_TOKEN }}
          aws_default_region: ${{ secrets.AWS_DEFAULT_REGION }}
          sccache_s3_bucket: ${{ secrets.SCCACHE_S3_BUCKET }}
          aws_account_id: ${{ secrets.AWS_ACCOUNT_ID }}
          aws_access_key_id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws_secret_access_key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}

      - name: Tag and Push vllm image to ACR
        uses: ./.github/actions/docker-tag-push
        with:
          local_image: ${{ steps.build-vllm.outputs.image_tag }}
          push_tag: ${{ secrets.AZURE_ACR_HOSTNAME }}/ai-dynamo/dynamo:${{ env.VLLM_IMAGE_TAG }}
          aws_push: "false"
          azure_push: "true"
          aws_account_id: ${{ secrets.AWS_ACCOUNT_ID }}
          aws_default_region: ${{ secrets.AWS_DEFAULT_REGION }}
          azure_acr_hostname: ${{ secrets.AZURE_ACR_HOSTNAME }}
          azure_acr_user: ${{ secrets.AZURE_ACR_USER }}
          azure_acr_password: ${{ secrets.AZURE_ACR_PASSWORD }}

      - name: Configure kubeconfig and namespace
        shell: bash
        run: |
          set -euo pipefail
          echo "${{ secrets.AZURE_AKS_CI_KUBECONFIG_B64 }}" | base64 -d > .kubeconfig
          chmod 600 .kubeconfig
          export KUBECONFIG=$(pwd)/.kubeconfig
          # Create ephemeral namespace for Dynamo
          PROFILE_SANITIZED="${PROFILE//_/-}"
          echo "NAMESPACE=gh-job-id-${{ github.run_id }}-${FRAMEWORK}-${PROFILE_SANITIZED}" >> $GITHUB_ENV
          kubectl delete namespace gh-job-id-${{ github.run_id }}-${FRAMEWORK}-${PROFILE_SANITIZED} || true
          kubectl create namespace gh-job-id-${{ github.run_id }}-${FRAMEWORK}-${PROFILE_SANITIZED}
          kubectl label namespaces gh-job-id-${{ github.run_id }}-${FRAMEWORK}-${PROFILE_SANITIZED} \
            nscleanup/enabled=true nscleanup/ttl=7200 gitlab-imagepull=enabled ngc-api=enabled nvcr-imagepull=enabled --overwrite=true
          kubectl config set-context --current --namespace=gh-job-id-${{ github.run_id }}-${FRAMEWORK}-${PROFILE_SANITIZED} --kubeconfig "$KUBECONFIG"

      - name: Install Dynamo Operator Helm chart
        shell: bash
        env:
          KUBECONFIG: ${{ github.workspace }}/.kubeconfig
        run: |
          set -euo pipefail
          helm repo add bitnami https://charts.bitnami.com/bitnami
          cd deploy/cloud/helm/platform/
          helm dep build .
          helm upgrade --install dynamo-platform . --namespace ${NAMESPACE} \
            --set dynamo-operator.namespaceRestriction.enabled=true \
            --set dynamo-operator.namespaceRestriction.allowedNamespaces[0]=${NAMESPACE} \
            --set dynamo-operator.controllerManager.manager.image.repository=${{ secrets.AZURE_ACR_HOSTNAME }}/ai-dynamo/dynamo \
            --set dynamo-operator.controllerManager.manager.image.tag=${{ env.OPERATOR_IMAGE_TAG }} \
            --set dynamo-operator.imagePullSecrets[0].name=docker-imagepullsecret
          timeout 300s kubectl rollout status deployment -n ${NAMESPACE} --watch || true

      - name: Configure image pull secrets and deploy vllm graph
        shell: bash
        env:
          KUBECONFIG: ${{ github.workspace }}/.kubeconfig
        run: |
          set -euo pipefail
          export KUBE_NS=${NAMESPACE}
          export FRAMEWORK_RUNTIME_IMAGE="${{ secrets.AZURE_ACR_HOSTNAME }}/ai-dynamo/dynamo:${VLLM_IMAGE_TAG}"
          # Secrets
          kubectl create secret generic hf-token-secret --from-literal=HF_TOKEN=${{ secrets.HF_TOKEN }} -n ${KUBE_NS} || true
          kubectl create secret docker-registry docker-imagepullsecret \
            --docker-server=${{ secrets.AZURE_ACR_HOSTNAME }} \
            --docker-username=${{ secrets.AZURE_ACR_USER }} \
            --docker-password=${{ secrets.AZURE_ACR_PASSWORD }} \
            --namespace=${KUBE_NS} || true
          # Deploy vllm backend graph
          cd $GITHUB_WORKSPACE/components/backends/${FRAMEWORK}
          export GRAPH_NAME=$(yq e '.metadata.name' ${DEPLOYMENT_FILE})
          yq -i '.spec.services.[].extraPodSpec.mainContainer.image = env(FRAMEWORK_RUNTIME_IMAGE)' ${DEPLOYMENT_FILE}
          kubectl apply -n ${KUBE_NS} -f ${DEPLOYMENT_FILE}
          sleep 20
          kubectl wait --for=condition=ready pod -l "nvidia.com/dynamo-graph-deployment-name=${GRAPH_NAME}" -n ${KUBE_NS} --timeout=1000s

      - name: Install Gateway API (standard)
        shell: bash
        env:
          KUBECONFIG: ${{ github.workspace }}/.kubeconfig
        run: |
          set -euo pipefail
          kubectl apply -f https://github.com/kubernetes-sigs/gateway-api/releases/download/${GATEWAY_API_VERSION}/standard-install.yaml

      - name: Install Inference Extension CRDs
        shell: bash
        env:
          KUBECONFIG: ${{ github.workspace }}/.kubeconfig
        run: |
          set -euo pipefail
          # Note: manifests are cluster-scoped; namespace flag kept to match request
          kubectl apply -f https://github.com/kubernetes-sigs/gateway-api-inference-extension/releases/download/${INFERENCE_EXTENSION_VERSION}/manifests.yaml -n my-model

      - name: Install Kgateway CRDs and controller
        shell: bash
        env:
          KUBECONFIG: ${{ github.workspace }}/.kubeconfig
        run: |
          set -euo pipefail
          helm upgrade -i --create-namespace --namespace kgateway-system --version ${KGATEWAY_VERSION} kgateway-crds oci://cr.kgateway.dev/kgateway-dev/charts/kgateway-crds
          helm upgrade -i --namespace kgateway-system --version ${KGATEWAY_VERSION} kgateway oci://cr.kgateway.dev/kgateway-dev/charts/kgateway --set inferenceExtension.enabled=true

      - name: Deploy Gateway Instance
        shell: bash
        env:
          KUBECONFIG: ${{ github.workspace }}/.kubeconfig
        run: |
          set -euo pipefail
          kubectl create namespace my-model || true
          kubectl apply -f https://github.com/kubernetes-sigs/gateway-api-inference-extension/raw/main/config/manifests/gateway/kgateway/gateway.yaml -n my-model

      - name: Clone GAIE repo and set environment
        shell: bash
        run: |
          set -euo pipefail
          git clone https://github.com/kubernetes-sigs/gateway-api-inference-extension.git
          cd gateway-api-inference-extension
          git checkout ${INFERENCE_EXTENSION_VERSION}
          echo "GAIE_DIR=$(pwd)" >> $GITHUB_ENV
          echo "DYNAMO_DIR=${{ github.workspace }}" >> $GITHUB_ENV

      - name: Run build-epp-dynamo.sh
        shell: bash
        env:
          GAIE_DIR: ${{ env.GAIE_DIR }}
          DYNAMO_DIR: ${{ env.DYNAMO_DIR }}
        run: |
          set -euo pipefail
          cd ${DYNAMO_DIR}/deploy/inference-gateway
          ./build-epp-dynamo.sh
          docker tag us-central1-docker.pkg.dev/k8s-staging-images/gateway-api-inference-extension/epp:v0.5.1-dirty gitlab-master.nvidia.com:5005/dl/ai-dynamo/dynamo/epp-inference-extension-dynamo:${{ github.run_id }}
          docker push gitlab-master.nvidia.com:5005/dl/ai-dynamo/dynamo/epp-inference-extension-dynamo:${{ github.run_id }}

      - name: Deploy GAIE Helm chart with built EPP image
        shell: bash
        env:
          KUBECONFIG: ${{ github.workspace }}/.kubeconfig
          GAIE_DIR: ${{ env.GAIE_DIR }}
          DYNAMO_DIR: ${{ env.DYNAMO_DIR }}
        run: |
          set -euo pipefail
          # Discover the EPP image tag from the previous build
          cd "${GAIE_DIR}"
          EPP_IMAGE="$(docker images --format '{{.Repository}}:{{.Tag}}' | grep 'gateway-api-inference-extension/epp' | head -n1 || true)"
          if [[ -z "${EPP_IMAGE}" ]]; then
            # Fallback: reconstruct the default tag used by the Makefile
            GIT_TAG=$(git describe --tags --dirty --always)
            EPP_IMAGE="us-central1-docker.pkg.dev/k8s-staging-images/gateway-api-inference-extension/epp:${GIT_TAG}"
          fi
          echo "Using EPP_IMAGE=${EPP_IMAGE}"
          export EPP_IMAGE

          # Deploy Helm chart in the Dynamo repo pointing to the EPP image
          cd "${DYNAMO_DIR}/deploy/inference-gateway"
          helm upgrade --install dynamo-gaie ./helm/dynamo-gaie -n my-model -f ./vllm_agg_qwen.yaml --set-string extension.image=${EPP_IMAGE}

      - name: Verify qwen-epp deployment readiness in my-model
        shell: bash
        env:
          KUBECONFIG: ${{ github.workspace }}/.kubeconfig
        run: |
          set -euo pipefail
          NS=my-model
          echo "Waiting for Deployment/qwen-epp to appear in namespace ${NS}..."
          ATTEMPTS=60
          SLEEP=10
          FOUND=0
          for i in $(seq 1 ${ATTEMPTS}); do
            if kubectl get deploy qwen-epp -n ${NS} >/dev/null 2>&1; then
              FOUND=1
              break
            fi
            sleep ${SLEEP}
          done
          if [[ ${FOUND} -ne 1 ]]; then
            echo "Deployment qwen-epp not found in ${NS} after $((ATTEMPTS*SLEEP)) seconds" >&2
            kubectl get deploy -n ${NS} || true
            exit 1
          fi
          echo "Deployment qwen-epp found. Waiting for rollout to complete..."
          kubectl rollout status deploy/qwen-epp -n ${NS} --timeout=600s
          kubectl get deploy qwen-epp -n ${NS} -o wide

      - name: Cleanup (optional)
        if: always()
        shell: bash
        env:
          KUBECONFIG: ${{ github.workspace }}/.kubeconfig
        run: |
          set -euo pipefail
          kubectl get all -n ${NAMESPACE} || true
          helm uninstall dynamo-platform -n ${NAMESPACE} || true
          kubectl delete dynamographdeployments --all -n ${NAMESPACE} || true
          kubectl delete namespace ${NAMESPACE} || true
          # GAIE related namespaces
          kubectl get all -n my-model || true
          kubectl delete namespace my-model || true
