# SPDX-FileCopyrightText: Copyright (c) 2024-2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0

name: Docker Build and Test

on:
  push:
    branches:
      - main
      - "pull-request/[0-9]+"

concurrency:
    group: ${{ github.workflow }}-build-test-${{ github.ref_name || github.run_id }}
    cancel-in-progress: ${{ github.ref != 'refs/heads/main' }}

jobs:
  changed-files:
    runs-on: ubuntu-latest
    outputs:
      has_code_changes: ${{ steps.filter.outputs.has_code_changes }}
    steps:
      - name: Checkout code
        uses: actions/checkout@08eba0b27e820071cde6df949e0beb9ba4906955  # v4.3.0
      - name: Check for changes
        uses: dorny/paths-filter@de90cc6fb38fc0963ad72b210f1f284cd68cea36  # v3.0.2
        id: filter
        with:
          filters: .github/filters.yaml

  # backend-status-check:
  #   runs-on: ubuntu-latest
  #   needs: [vllm, sglang, trtllm, vllm-e2e,] # trtllm-e2e, sglang-e2e]
  #   if: always()
  #   steps:
  #     - name: "Check all dependent jobs"
  #       run: |
  #         echo '${{ toJson(needs) }}' | jq -e 'to_entries | map(.value.result) | all(. as $result | ["success", "skipped"] | any($result == .))'

  # vllm:
  #   runs-on: gpu-l40-amd64
  #   needs: changed-files
  #   if: needs.changed-files.outputs.has_code_changes == 'true'
  #   steps:
  #     - name: Checkout code
  #       uses: actions/checkout@08eba0b27e820071cde6df949e0beb9ba4906955  # v4.3.0
  #     - name: Build Container
  #       id: build-image
  #       uses: ./.github/actions/docker-build
  #       with:
  #         framework: vllm
  #         target: runtime
  #         ngc_ci_access_token: ${{ secrets.NGC_CI_ACCESS_TOKEN }}
  #         ci_token: ${{ secrets.CI_TOKEN }}
  #         aws_default_region: ${{ secrets.AWS_DEFAULT_REGION }}
  #         sccache_s3_bucket: ${{ secrets.SCCACHE_S3_BUCKET }}
  #         aws_access_key_id: ${{ secrets.AWS_ACCESS_KEY_ID }}
  #         aws_secret_access_key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
  #     - name: Docker Tag and Push
  #       uses: ./.github/actions/docker-tag-push
  #       with:
  #         local_image: ${{ steps.build-image.outputs.image_tag }}
  #         push_tag: ai-dynamo/dynamo:${{ github.sha }}-vllm-amd64
  #         aws_push: 'true'
  #         azure_push: 'true'
  #         aws_account_id: ${{ secrets.AWS_ACCOUNT_ID }}
  #         aws_default_region: us-west-2
  #         azure_acr_hostname: ${{ secrets.AZURE_ACR_HOSTNAME }}
  #         azure_acr_user: ${{ secrets.AZURE_ACR_USER }}
  #         azure_acr_password: ${{ secrets.AZURE_ACR_PASSWORD }}
  #     - name: Run tests
  #       uses: ./.github/actions/pytest
  #       with:
  #         image_tag: ${{ steps.build-image.outputs.image_tag }}
  #         pytest_marks: "e2e and vllm and gpu_1 and not slow"

  # sglang:
  #   runs-on: gpu-l40-amd64
  #   needs: changed-files
  #   if: needs.changed-files.outputs.has_code_changes == 'true'
  #   steps:
  #     - name: Checkout repository
  #       uses: actions/checkout@08eba0b27e820071cde6df949e0beb9ba4906955  # v4.3.0
  #     - name: Build Container
  #       id: build-image
  #       uses: ./.github/actions/docker-build
  #       with:
  #         framework: sglang
  #         target: runtime
  #         ngc_ci_access_token: ${{ secrets.NGC_CI_ACCESS_TOKEN }}
  #         ci_token: ${{ secrets.CI_TOKEN }}
  #         aws_default_region: ${{ secrets.AWS_DEFAULT_REGION }}
  #         sccache_s3_bucket: ${{ secrets.SCCACHE_S3_BUCKET }}
  #         aws_access_key_id: ${{ secrets.AWS_ACCESS_KEY_ID }}
  #         aws_secret_access_key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
  #     - name: Docker Tag and Push
  #       uses: ./.github/actions/docker-tag-push
  #       with:
  #         local_image: ${{ steps.build-image.outputs.image_tag }}
  #         push_tag: ai-dynamo/dynamo:${{ github.sha }}-sglang-amd64
  #         aws_push: 'true'
  #         azure_push: 'true'
  #         aws_account_id: ${{ secrets.AWS_ACCOUNT_ID }}
  #         aws_default_region: us-west-2
  #         azure_acr_hostname: ${{ secrets.AZURE_ACR_HOSTNAME }}
  #         azure_acr_user: ${{ secrets.AZURE_ACR_USER }}
  #         azure_acr_password: ${{ secrets.AZURE_ACR_PASSWORD }}
  #     - name: Run tests
  #       uses: ./.github/actions/pytest
  #       with:
  #         image_tag: ${{ steps.build-image.outputs.image_tag }}
  #         pytest_marks: "e2e and sglang and gpu_1"

  # trtllm:
  #   runs-on: gpu-l40-amd64
  #   needs: changed-files
  #   if: needs.changed-files.outputs.has_code_changes == 'true'
  #   steps:
  #     - name: Checkout code
  #       uses: actions/checkout@08eba0b27e820071cde6df949e0beb9ba4906955  # v4.3.0
  #     - name: Build Container
  #       id: build-image
  #       uses: ./.github/actions/docker-build
  #       with:
  #         framework: trtllm
  #         target: runtime
  #         ngc_ci_access_token: ${{ secrets.NGC_CI_ACCESS_TOKEN }}
  #         ci_token: ${{ secrets.CI_TOKEN }}
  #         aws_default_region: ${{ secrets.AWS_DEFAULT_REGION }}
  #         sccache_s3_bucket: ${{ secrets.SCCACHE_S3_BUCKET }}
  #         aws_access_key_id: ${{ secrets.AWS_ACCESS_KEY_ID }}
  #         aws_secret_access_key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
  #     - name: Docker Tag and Push
  #       uses: ./.github/actions/docker-tag-push
  #       with:
  #         local_image: ${{ steps.build-image.outputs.image_tag }}
  #         push_tag: ai-dynamo/dynamo:${{ github.sha }}-trtllm-amd64
  #         aws_push: 'true'
  #         azure_push: 'true'
  #         aws_account_id: ${{ secrets.AWS_ACCOUNT_ID }}
  #         aws_default_region: us-west-2
  #         azure_acr_hostname: ${{ secrets.AZURE_ACR_HOSTNAME }}
  #         azure_acr_user: ${{ secrets.AZURE_ACR_USER }}
  #         azure_acr_password: ${{ secrets.AZURE_ACR_PASSWORD }}
  #     - name: Run tests
  #       uses: ./.github/actions/pytest
  #       with:
  #         image_tag: ${{ steps.build-image.outputs.image_tag }}
  #         pytest_marks: "e2e and trtllm_marker and gpu_1 and not slow"

  build-operator:
    runs-on: gpu-l40-amd64
    steps:
      - name: Checkout code
        uses: actions/checkout@08eba0b27e820071cde6df949e0beb9ba4906955  # v4.3.0
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
      - name: Build Container
        id: build-image
        run: |
          cd deploy/cloud/operator
          docker build -f Dockerfile -t dynamo-operator:latest .
      - name: Docker Tag and Push
        uses: ./.github/actions/docker-tag-push
        with:
          local_image: dynamo-operator:latest
          push_tag: ai-dynamo/dynamo:${{ github.sha }}-operator-amd64
          aws_push: 'true'
          azure_push: 'true'
          aws_account_id: ${{ secrets.AWS_ACCOUNT_ID }}
          aws_default_region: us-west-2
          azure_acr_hostname: ${{ secrets.AZURE_ACR_HOSTNAME }}
          azure_acr_user: ${{ secrets.AZURE_ACR_USER }}
          azure_acr_password: ${{ secrets.AZURE_ACR_PASSWORD }}

  deploy-operator:
    runs-on: cpu-amd-m5-2xlarge
    needs: [build-operator]
    env:
      NAMESPACE: gh-job-id-${{ github.run_id }}
      DYNAMO_INGRESS_SUFFIX: dev.aire.nvidia.com
    outputs:
      namespace: ${{ env.NAMESPACE }}
    permissions:
      contents: read
    steps:
    - uses: actions/checkout@v4
    - name: Deploy Operator
      run: |
        set -x
        # Install dependencies
        sudo apt-get update && sudo apt-get install -y curl bash openssl gettext git jq

        # # TODO:
        # set -x
        # git clone -b $DYNAMO_REPO_BRANCH --depth=1 --single-branch https://dynamo-ai/dynamo dynamo
        # # use nvidia internal bitnami charts to avoid rate limiting
        # find dynamo -type f -exec sed -i 's|https://charts.bitnami.com/bitnami|oci://dockerhub.nvidia.com/bitnamicharts|g' {} +
        # echo $DYNAMO_REPO_CI_COMMIT_SHA | tee -a build.env

        # WE MOST LIKELY DON'T NEED uv
        # curl -LsSf https://astral.sh/uv/install.sh | sh
        # export PATH="/root/.local/bin:/opt/dynamo/venv/bin:$PATH"
        # uv sync

        # Install yq
        echo "Installing yq..."
        curl -L https://github.com/mikefarah/yq/releases/latest/download/yq_linux_amd64 -o yq
        sudo chmod 755 yq
        sudo mv yq /usr/local/bin/
        # Install Helm
        echo "Installing Helm..."
        curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
        sudo chmod 700 get_helm.sh
        sudo ./get_helm.sh
        # Install kubectl
        curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
        sudo chmod 755 kubectl
        sudo mv kubectl /usr/local/bin/

        # Make sure the right context is used
        echo "${{ secrets.AZURE_AKS_CI_KUBECONFIG_B64 }}" | base64 -d > .kubeconfig
        chmod 600 .kubeconfig
        export KUBECONFIG=$(pwd)/.kubeconfig
        kubectl config set-context --current --namespace=$NAMESPACE --kubeconfig "${KUBECONFIG}"
        kubectl config current-context

        # Create a namespace for this job
        echo "Creating an ephemeral namespace..."
        kubectl delete namespace $NAMESPACE || true
        kubectl create namespace $NAMESPACE || true
        echo "Attaching the labels for secrets and cleanup"
        kubectl label namespaces ${NAMESPACE} nscleanup/enabled=true nscleanup/ttl=7200 gitlab-imagepull=enabled ngc-api=enabled nvcr-imagepull=enabled --overwrite=true

        # Set the namespace as default
        kubectl config set-context --current --namespace=$NAMESPACE

        # Get helm repo for platform helm chart
        # helm repo add --username gitlab-ci-token --password ${CI_JOB_TOKEN} ${CI_PROJECT_NAME} ${CI_API_V4_URL}/projects/${CI_PROJECT_ID}/packages/helm/dev
        # helm repo update

        # Check if Istio is installed
        kubectl get pods -n istio-system
        # Check if default storage class exists
        kubectl get storageclass

        # Install Helm chart
        # cat build.env
        export IMAGE_TAG=$(cat build.env)
        echo $IMAGE_TAG
        export VIRTUAL_ENV=/opt/dynamo/venv
        export KUBE_NS=$NAMESPACE
        export ISTIO_ENABLED=true
        export ISTIO_GATEWAY=istio-system/ingress-alb
        export VIRTUAL_SERVICE_SUPPORTS_HTTPS=true
        export DYNAMO_CLOUD=https://${NAMESPACE}.${DYNAMO_INGRESS_SUFFIX}
        # export DOCKER_SERVER=$CI_REGISTRY_IMAGE
        # export DOCKER_USERNAME=gitlab-ci-token
        # export DOCKER_PASSWORD=${CI_JOB_TOKEN}
        kubectl config current-context

        # Install dynamo env secrets
        kubectl create secret generic hf-token-secret --from-literal=HF_TOKEN=$HF_TOKEN -n $KUBE_NS || true
        # Install CRDs
        # helm upgrade --install dynamo-crds ./deploy/cloud/helm/crds/ --namespace default --wait --atomic
        # Create docker pull secret for operator image
        kubectl create secret docker-registry docker-imagepullsecret --docker-server=${{ secrets.AZURE_ACR_HOSTNAME }} --docker-username=${{ secrets.AZURE_ACR_USER }} --docker-password=${{ secrets.AZURE_ACR_PASSWORD }} --namespace=${NAMESPACE}
        # Install helm dependencies
        helm repo add bitnami https://charts.bitnami.com/bitnami
        cd deploy/cloud/helm/platform/
        helm dep build .
        # Install platform
        helm upgrade --install dynamo-platform . --namespace ${NAMESPACE} --set dynamo-operator.namespaceRestriction.enabled=true --set dynamo-operator.controllerManager.manager.image.repository=${{ secrets.AZURE_ACR_HOSTNAME }}/ai-dynamo/dynamo --set dynamo-operator.controllerManager.manager.image.tag=${{ github.sha }}-operator-amd64 --set dynamo-operator.imagePullSecrets[0].name=docker-imagepullsecret
        # Wait for all deployments to be ready
        timeout 300s kubectl rollout status deployment -n $NAMESPACE --watch
        cd -

  vllm-e2e:
    runs-on: cpu-amd-m5-2xlarge
    needs: [deploy-operator]
    env:
      NAMESPACE: ${{ needs.deploy-operator.outputs.namespace }}
      DEPLOYMENT_FILE: "deploy/agg.yaml"
    permissions:
      contents: read
    steps:
    - uses: actions/checkout@v4
    - name: Install dependencies
      run: |
        set -x
        # Install yq
        echo "Installing yq..."
        curl -L https://github.com/mikefarah/yq/releases/latest/download/yq_linux_amd64 -o yq
        sudo chmod 755 yq
        sudo mv yq /usr/local/bin/
        # Install kubectl
        echo "Installing kubectl..."
        curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
        sudo chmod 755 kubectl
        sudo mv kubectl /usr/local/bin/
    - name: Setup kubeconfig
      run: |
        set -x
        # Setup kubeconfig
        echo "${{ secrets.AZURE_AKS_CI_KUBECONFIG_B64 }}" | base64 -d > .kubeconfig
        chmod 600 .kubeconfig
        export KUBECONFIG=$(pwd)/.kubeconfig
        kubectl config set-context --current --namespace=$NAMESPACE --kubeconfig "${KUBECONFIG}"
        kubectl config current-context
    - name: Run e2e Tests
      run: |
        set -x
        # Setup kubeconfig for this step
        echo "${{ secrets.AZURE_AKS_CI_KUBECONFIG_B64 }}" | base64 -d > .kubeconfig
        chmod 600 .kubeconfig
        export KUBECONFIG=$(pwd)/.kubeconfig
        kubectl config set-context --current --namespace=$NAMESPACE --kubeconfig "${KUBECONFIG}"

        cd components/backends/vllm
        export FRAMEWORK_RUNTIME_IMAGE="${{ secrets.AZURE_ACR_HOSTNAME }}/ai-dynamo/dynamo:cb6e511109e95d63eeb348107e5c1d0be8d5346b-vllm-amd64"
        export KUBE_NS=$NAMESPACE
        export GRAPH_NAME=$(yq e '.metadata.name' $DEPLOYMENT_FILE)
        yq '.spec.services.[].extraPodSpec.mainContainer.image = env(FRAMEWORK_RUNTIME_IMAGE)' $DEPLOYMENT_FILE | kubectl apply -n $KUBE_NS -f -
        # --- Wait for the frontend pod to be ready ---
        sleep 20
        export POD_NAME=$(kubectl get pods -n ${KUBE_NS} | grep "${GRAPH_NAME}" | grep "worker" | sort -k1 | tail -n1 | awk '{print $1}')
        kubectl wait --for=condition=ready pod/$POD_NAME --timeout=1000s
        kubectl get all -n $KUBE_NS
  

  # trtllm-e2e:
  #   runs-on: cpu-amd-m5-2xlarge
  #   needs: [deploy-operator, trtllm]
  #   env:
  #     NAMESPACE: ${{ needs.deploy-operator.outputs.namespace }}
  #   permissions:
  #     contents: read
  #   steps:
  #   - uses: actions/checkout@v4
  #   - name: Run TRT-LLM Tests
  #     uses: ./.github/actions/pytest
  #     with:
  #       image_tag: ai-dynamo/dynamo:${{ github.sha }}-trtllm-amd64
  #       pytest_marks: "e2e and trtllm_marker and gpu_1 and not slow"

  # sglang-e2e:
  #   runs-on: cpu-amd-m5-2xlarge
  #   needs: [deploy-operator, sglang]
  #   env:
  #     NAMESPACE: ${{ needs.deploy-operator.outputs.namespace }}
  #   permissions:
  #     contents: read
  #   steps:
  #   - uses: actions/checkout@v4
  #   - name: Run SGLang Tests
  #     uses: ./.github/actions/pytest
  #     with:
  #       image_tag: ai-dynamo/dynamo:${{ github.sha }}-sglang-amd64
  #       pytest_marks: "e2e and sglang and gpu_1"

  cleanup-operator:
    runs-on: cpu-amd-m5-2xlarge
    needs: [deploy-operator, vllm-e2e,] # trtllm-e2e, sglang-e2e]
    if: always()
    env:
      NAMESPACE: ${{ needs.deploy-operator.outputs.namespace }}
    permissions:
      contents: read
    steps:
    - uses: actions/checkout@v4
    - name: Cleanup Operator
      timeout-minutes: 10
      run: |
        echo "${{ secrets.AZURE_AKS_CI_KUBECONFIG_B64 }}" | base64 -d > .kubeconfig
        chmod 600 .kubeconfig
        export KUBECONFIG=$(pwd)/.kubeconfig
        kubectl config set-context --current --namespace=$NAMESPACE --kubeconfig "${KUBECONFIG}"

        # For debugging purposes, list all the resources before we uninstall
        kubectl get all

        echo "Starting cleanup... after 300 seconds"
        sleep 300

        echo "Deleting all DynamoGraphDeployments in namespace $NAMESPACE..."
        kubectl delete dynamographdeployments --all -n $NAMESPACE || true

        # Uninstall the helm chart
        helm ls
        helm uninstall dynamo-platform || true

        echo "Namespace $NAMESPACE deletion initiated, proceeding with cleanup..."
        timeout 7200s kubectl delete namespace $NAMESPACE || true
        echo "Namespace $NAMESPACE completed."
