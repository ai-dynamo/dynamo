name: 'Pytest Local'
description: 'Run pytest locally'
inputs:
  pytest_marks:
    description: 'Pytest marks'
    required: true
    default: 'e2e and vllm and gpu_1 and not slow'
  cpu_limit:
    description: 'Maximum number of cores available to docker'
    required: false
    default: '10'
  framework:
    description: 'Framework name for test metrics'
    required: false
    default: 'unknown'
  test_type:
    description: 'Test type (unit, e2e, integration)'
    required: false
    default: 'e2e'
  platform_arch:
    description: 'Platform architecture (amd64, arm64)'
    required: false
    default: 'amd64'
  dry_run:
    description: 'Run pytest in dry-run mode (collect tests only, do not execute)'
    required: false
    default: 'false'


runs:
  using: "composite"
  steps:
    - name: Setup Test Environment
      shell: bash
      run: |
        # Setup test directories
        # mkdir -p test-results

        # Set platform architecture from input
        PLATFORM_ARCH="${{ inputs.platform_arch }}"
        if [[ -z "${PLATFORM_ARCH}" ]]; then
          PLATFORM_ARCH="amd64"
        fi
        echo "PLATFORM_ARCH=${PLATFORM_ARCH}" >> $GITHUB_ENV
        echo "üèóÔ∏è  Platform architecture: ${PLATFORM_ARCH}"


    - name: Run tests
      shell: bash
      env:
        NUM_CPUS: ${{ inputs.cpu_limit }}
        CONTAINER_ID: test_${{ github.run_id }}_${{ github.run_attempt }}_${{ github.job }}
        PYTEST_XML_FILE: pytest_test_report.xml
        HF_HOME: /runner/_work/_temp
      run: |
        # 1. Install uv and fix PATH
        curl -LsSf https://astral.sh/uv/install.sh | sh
        echo "$HOME/.cargo/bin" >> $GITHUB_PATH
        export PATH="$HOME/.cargo/bin:$PATH"

        # 2. Setup Virtual Env (Using default .venv for 'uv run' compatibility)
        uv venv
        source .venv/bin/activate

        # 3. Install dependencies
        uv pip install pytest
        uv pip install --no-cache -e .

        set +e

        # 4. Define Test Directory
        TEST_RESULTS_DIR="$(pwd)/test-results"
        mkdir -p "$TEST_RESULTS_DIR"

        # 5. Build Command (Fixed JUnit path to use the local results dir)
        if [[ "${{ inputs.dry_run }}" == "true" ]]; then
          echo "üîç Running pytest in dry-run mode"
          PYTEST_CMD="uv run pytest -v --collect-only -m \"${{ inputs.pytest_marks }}\""
        else
          echo "üöÄ Running pytest in normal mode"
          # FIXED: Changed /workspace/test-results to ./test-results
          PYTEST_CMD="uv run pytest -v --tb=short --basetemp=/tmp -o cache_dir=/tmp/.pytest_cache --junitxml=test-results/${{ env.PYTEST_XML_FILE }} --durations=10 -m \"${{ inputs.pytest_marks }}\""

          if command -v nvidia-smi &> /dev/null && nvidia-smi &> /dev/null; then
            echo "‚úì GPU detected"
            GPU_FLAGS="--runtime=nvidia --gpus all"
          fi
        fi

        # 6. Execute
        bash -c "${PYTEST_CMD}"

        TEST_EXIT_CODE=$?
        echo "TEST_EXIT_CODE=${TEST_EXIT_CODE}" >> $GITHUB_ENV
        echo "üß™ Tests completed with exit code: ${TEST_EXIT_CODE}"

        if [[ "${{ inputs.dry_run }}" != "true" ]]; then
          if [[ -f "test-results/${{ env.PYTEST_XML_FILE }}" ]]; then
            echo "‚úÖ Test results file found."
          else
            echo "‚ö†Ô∏è  Test results file NOT found."
          fi
        fi

        exit 0

    - name: Process Test Results
      shell: bash
      run: |

        # Sanitize test_type for filenames (always set this for artifact upload)
        # Remove commas and spaces from test_type for use in filenames
        STR_TEST_TYPE=$(echo "${{ inputs.test_type }}" | tr ', ' '_')
        echo "STR_TEST_TYPE=${STR_TEST_TYPE}" >> $GITHUB_ENV

        # Check for JUnit XML file and determine test status
        JUNIT_FILE="test-results/pytest_test_report.xml"

        if [[ -f "$JUNIT_FILE" ]]; then
          echo "‚úÖ JUnit XML generated successfully"
          # Extract basic test counts for status determination
          TOTAL_TESTS=$(grep -o 'tests="[0-9]*"' "$JUNIT_FILE" | grep -o '[0-9]*' | head -1 || echo "0")
          FAILED_TESTS=$(grep -o 'failures="[0-9]*"' "$JUNIT_FILE" | grep -o '[0-9]*' | head -1 || echo "0")
          ERROR_TESTS=$(grep -o 'errors="[0-9]*"' "$JUNIT_FILE" | grep -o '[0-9]*' | head -1 || echo "0")
          echo "üìä ${TOTAL_TESTS} tests completed (${FAILED_TESTS} failed, ${ERROR_TESTS} errors)"

          # Rename XML file to unique name
          JUNIT_NAME="pytest_test_report_${{ inputs.framework }}_${STR_TEST_TYPE}_${{ inputs.platform_arch }}_${{ github.run_id }}_${{ job.check_run_id }}.xml"
          mv "$JUNIT_FILE" "test-results/$JUNIT_NAME"
          echo "üìù Renamed XML file to: $JUNIT_NAME"
        else
          echo "‚ö†Ô∏è  JUnit XML file not found - test results may not be available for upload"
          TOTAL_TESTS=0
          FAILED_TESTS=1  # Treat missing XML as failure
          ERROR_TESTS=0
        fi

        # Exit with original test result to maintain workflow behavior
        exit ${TEST_EXIT_CODE}

    - name: Upload Test Results
      uses: actions/upload-artifact@v4
      if: always()  # Always upload test results, even if tests failed
      with:
        name: test-results-${{ inputs.framework }}-${{ env.STR_TEST_TYPE }}-${{ env.PLATFORM_ARCH }}-${{ github.run_id }}-${{ job.check_run_id }}
        path: test-results/pytest_test_report_${{ inputs.framework }}_${{ env.STR_TEST_TYPE }}_${{ inputs.platform_arch }}_${{ github.run_id }}_${{ job.check_run_id }}.xml
        retention-days: 7
