name: 'Pytest'
description: 'Run pytest on pre-built container images'
inputs:
  pytest_marks:
    description: 'Pytest marks'
    required: true
    default: 'e2e and vllm and gpu_1 and not slow'
  image_tag:
    description: 'Image Tag to run tests on'
    required: true
  cpu_limit:
    description: 'Maximum number of cores available to docker'
    required: false
    default: '10'
  framework:
    description: 'Framework name for test metrics'
    required: false
    default: 'unknown'
  test_type:
    description: 'Test type (unit, e2e, integration)'
    required: false
    default: 'e2e'


runs:
  using: "composite"
  steps:
    - name: Setup Test Environment
      shell: bash
      run: |
        # Setup test directories
        mkdir -p test-results
        
        # Extract platform architecture from job context
        PLATFORM_ARCH="amd64"  # Default
        if [[ "${{ github.job }}" == *"arm64"* ]]; then
          PLATFORM_ARCH="arm64"
        fi
        echo "PLATFORM_ARCH=${PLATFORM_ARCH}" >> $GITHUB_ENV
        echo "🏗️  Platform architecture: ${PLATFORM_ARCH}"

    - name: Run tests
      shell: bash
      env:
        NUM_CPUS: ${{ inputs.cpu_limit }}
        CONTAINER_ID: test_${{ github.run_id }}_${{ github.run_attempt }}_${{ github.job }}
        PYTEST_XML_FILE: pytest_test_report.xml
        HF_HOME: /runner/_work/_temp
      run: |
        # Run pytest with detailed output and JUnit XML
        set +e  # Don't exit on test failures
        
        docker run --runtime=nvidia --rm --gpus all -w /workspace \
          --cpus=${NUM_CPUS} \
          --network host \
          --name ${{ env.CONTAINER_ID }}_pytest \
          -v "$(pwd)/test-results:/test-results" \
          ${{ inputs.image_tag }} \
          bash -c "pytest -v --tb=short --basetemp=/tmp --junitxml=/test-results/${{ env.PYTEST_XML_FILE }} --durations=10 -m \"${{ inputs.pytest_marks }}\""
        
        TEST_EXIT_CODE=$?
        echo "TEST_EXIT_CODE=${TEST_EXIT_CODE}" >> $GITHUB_ENV
        echo "🧪 Tests completed with exit code: ${TEST_EXIT_CODE}"
        
        # Always continue to results processing
        exit 0

    - name: Process Test Results
      shell: bash
      run: |
        
        # Parse JUnit XML for detailed test metrics
        JUNIT_FILE="test-results/pytest_test_report.xml"
        echo "🔍 Looking for JUnit XML at: ${JUNIT_FILE}"
        echo "📁 Contents of test-results directory:"
        ls -la test-results/ || echo "   Directory does not exist"
        TOTAL_TESTS=0
        PASSED_TESTS=0
        FAILED_TESTS=0
        SKIPPED_TESTS=0
        ERROR_TESTS=0
        
        if [[ -f "$JUNIT_FILE" ]]; then
          echo "📄 JUnit XML file found: $JUNIT_FILE"
          echo "📄 JUnit XML content (for debugging):"
          echo "--- START XML ---"
          cat "$JUNIT_FILE"
          echo "--- END XML ---"
          
          # Extract test counts from JUnit XML
          TOTAL_TESTS=$(grep -o 'tests="[0-9]*"' "$JUNIT_FILE" | grep -o '[0-9]*' | head -1 || echo "0")
          FAILED_TESTS=$(grep -o 'failures="[0-9]*"' "$JUNIT_FILE" | grep -o '[0-9]*' | head -1 || echo "0")
          ERROR_TESTS=$(grep -o 'errors="[0-9]*"' "$JUNIT_FILE" | grep -o '[0-9]*' | head -1 || echo "0")
          SKIPPED_TESTS=$(grep -o 'skipped="[0-9]*"' "$JUNIT_FILE" | grep -o '[0-9]*' | head -1 || echo "0")
          PASSED_TESTS=$((TOTAL_TESTS - FAILED_TESTS - ERROR_TESTS - SKIPPED_TESTS))
          
          echo "📊 Test Results:"
          echo "   Total: ${TOTAL_TESTS}"
          echo "   Passed: ${PASSED_TESTS}"
          echo "   Failed: ${FAILED_TESTS}"
          echo "   Errors: ${ERROR_TESTS}"
          echo "   Skipped: ${SKIPPED_TESTS}"
        else
          echo "⚠️  JUnit XML file not found: $JUNIT_FILE"
          echo "📁 Contents of test-results directory:"
          ls -la test-results/ || echo "   Directory does not exist"
          echo "📁 Looking for any XML files in test-results:"
          XML_FILES=$(find test-results/ -name "*.xml" 2>/dev/null)
          if [[ -n "$XML_FILES" ]]; then
            echo "$XML_FILES"
            echo "📄 Contents of found XML files:"
            for xml_file in $XML_FILES; do
              echo "--- START $xml_file ---"
              cat "$xml_file" 2>/dev/null || echo "Could not read $xml_file"
              echo "--- END $xml_file ---"
            done
          else
            echo "   No XML files found"
          fi
          echo "📁 Looking for any files with 'pytest' in the name:"
          PYTEST_FILES=$(find test-results/ -name "*pytest*" 2>/dev/null)
          if [[ -n "$PYTEST_FILES" ]]; then
            echo "$PYTEST_FILES"
            echo "📄 Contents of found pytest files:"
            for pytest_file in $PYTEST_FILES; do
              echo "--- START $pytest_file ---"
              cat "$pytest_file" 2>/dev/null || echo "Could not read $pytest_file"
              echo "--- END $pytest_file ---"
            done
          else
            echo "   No pytest files found"
          fi
        fi
        
        # Determine overall test status
        TEST_STATUS="success"
        if [[ "${TEST_EXIT_CODE}" != "0" ]] || [[ "${FAILED_TESTS}" != "0" ]] || [[ "${ERROR_TESTS}" != "0" ]]; then
          TEST_STATUS="failure"
        fi
        
        echo "📊 Test Results Summary:"
        echo "   Total: ${TOTAL_TESTS}"
        echo "   Passed: ${PASSED_TESTS}"
        echo "   Failed: ${FAILED_TESTS}"
        echo "   Errors: ${ERROR_TESTS}"
        echo "   Skipped: ${SKIPPED_TESTS}"
        echo "   Status: ${TEST_STATUS}"
        
        # Individual test data is now handled directly by the upload script parsing JUnit XML
        echo "✅ Test metrics collection completed - JUnit XML will be processed by upload script"
        
        # Exit with original test result to maintain workflow behavior
        exit ${TEST_EXIT_CODE}

    - name: Upload Test Results
      uses: actions/upload-artifact@v4
      if: always()  # Always upload test results, even if tests failed
      with:
        name: test-results-${{ inputs.framework }}-${{ inputs.test_type }}-${{ env.PLATFORM_ARCH }}
        path: test-results/${{ env.PYTEST_XML_FILE }}
        retention-days: 7